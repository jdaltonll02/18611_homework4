{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f3fe1f",
   "metadata": {},
   "source": [
    "# HW4: QA Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c91ec1",
   "metadata": {},
   "source": [
    "## Dependencies and LLM Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "42689839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain==1.0.5\n",
    "# !pip install langchain-core\n",
    "# !pip install langchain-community\n",
    "# !pip install faiss-cpu\n",
    "# !pip install kagglehub\n",
    "# !Install DuckDuckGo search dependency\n",
    "# !pip install -U ddgs\n",
    "device = \"cuda\"  # \"cpu\" or \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7af723c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the list of countries we are using (with their official languages)\n",
    "# Feel free to use it in your code\n",
    "list_of_countries = {}\n",
    "with open(\"countries_with_languages.tsv\", \"r\"  ) as f:\n",
    "    for line in f.readlines():\n",
    "        country, langs = line.strip().split(\"\\t\")\n",
    "        list_of_countries[country] = langs.split(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c06a7d",
   "metadata": {},
   "source": [
    "### Choice 1: OpenAI API\n",
    "\n",
    "The notebook's implementation is based on this.\n",
    "Feel free to change the model, and please keep track of your usage on the \"Usage\" page on [LiteLLM API webpage](https://ai-gateway.andrew.cmu.edu/ui/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b65e4808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5add67b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "import getpass, os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
    "openai_model_id = \"gpt-5\"\n",
    "openai_embmodel_id = \"azure/text-embedding-3-small\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=openai_model_id,\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=\"https://ai-gateway.andrew.cmu.edu/\"\n",
    ")\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=openai_embmodel_id,\n",
    "    api_key=os.environ['OPENAI_API_KEY'],\n",
    "    base_url='https://ai-gateway.andrew.cmu.edu/'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799b00ca",
   "metadata": {},
   "source": [
    "### Choice 2: Hugging Face Models\n",
    "\n",
    "You may also use Hugging Face models without API credits if you have available GPU resource. You might have to the change prompt templates according to your model choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ac1bc014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-huggingface text-generation transformers google-search-results \n",
    "# !pip install numexpr langchainhub sentencepiece sentence-transformers jinja2 bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bbf0fa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import getpass, os\n",
    "# from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline, HuggingFaceEmbeddings\n",
    "\n",
    "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass(\"Enter your Hugging Face API key: \")\n",
    "# hgf_model_id = \"Qwen/Qwen3-0.6B\"\n",
    "# hgf_embmodel_id = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# hgf_model = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=hgf_model_id,\n",
    "#     task=\"text-generation\",\n",
    "#     pipeline_kwargs=dict(\n",
    "#         max_new_tokens=128,\n",
    "#         do_sample=False,\n",
    "#     ),\n",
    "# )\n",
    "# hgf_llm = ChatHuggingFace(hgf_model)\n",
    "# hgf_embeddings = HuggingFaceEmbeddings(model_name=hgf_embmodel_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a37cbe",
   "metadata": {},
   "source": [
    "## Handling different type of questions\n",
    "\n",
    "Implement the answer formatting and extraction for each question type. You may change the prompt to fit your processing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9e242dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6a4262",
   "metadata": {},
   "source": [
    "### üó∫Ô∏èGlobal Trekker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "fc39e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hardened web_search: import inside try to avoid ddgs crash\n",
    "# from langchain.tools import tool\n",
    "\n",
    "# @tool\n",
    "# def web_search(query: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Searches the web for information about locations, landmarks, and geographic features.\n",
    "#     Useful for identifying cities and countries from descriptive clues.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         from langchain_community.tools import DuckDuckGoSearchRun\n",
    "#         search = DuckDuckGoSearchRun()\n",
    "#         try:\n",
    "#             results = search.run(query)\n",
    "#             return results\n",
    "#         except Exception as e:\n",
    "#             return f\"Search error: {str(e)}\"\n",
    "#     except Exception as e:\n",
    "#         return (\n",
    "#             \"DuckDuckGo search dependency missing or failed to load. \"\n",
    "#             \"Install with: pip install -U ddgs. Error: \" + str(e)\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1bce1dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_trekker_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert in world knowledge.\"},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"Given the following paragraph, guess the most likely country and city. Answer in the format of [country], [city]. If the paragraph is generic or only country-level, return \"Unknown\" for city.\"\"\"},\n",
    "]\n",
    "global_trekker = create_agent(model=llm, tools=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "85342fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_global_trekker_answer(response: str) -> tuple[str, str]:\n",
    "    # TODO: Extract the country and city from the response\n",
    "    # return country, city\n",
    "    import re\n",
    "    \n",
    "    chars = r'\\[([^\\]]+)\\]\\s*,\\s*\\[([^\\]]+)\\]'\n",
    "    match = re.search(chars, response)\n",
    "    \n",
    "    if match:\n",
    "        country = match.group(1).strip()\n",
    "        city = match.group(2).strip()\n",
    "        return country, city\n",
    "    \n",
    "    # Fallback: look for two items separated by comma\n",
    "    # Often LLMs will say something like \"United States, Pittsburgh\"\n",
    "    lines = response.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        # Skip lines that are too long (likely explanations)\n",
    "        if len(line) > 100:\n",
    "            continue\n",
    "        # Look for comma-separated values\n",
    "        if ',' in line:\n",
    "            parts = line.split(',')\n",
    "            if len(parts) >= 2:\n",
    "                country = parts[0].strip().strip('[]\"\\'')\n",
    "                city = parts[1].strip().strip('[]\"\\'')\n",
    "                # Clean up common prefixes\n",
    "                for prefix in ['The answer is', 'Answer:', 'Location:', 'Country:', 'City:']:\n",
    "                    country = country.replace(prefix, '').strip()\n",
    "                    city = city.replace(prefix, '').strip()\n",
    "                return country, city\n",
    "    \n",
    "    # Last resort: return empty strings\n",
    "    return \"\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "cd5210d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('AAA', 'BBB')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test run your extration function before using it in the main loop!\n",
    "extract_global_trekker_answer(\"AAA, BBB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78ab9c1",
   "metadata": {},
   "source": [
    "### üçΩÔ∏èCulinary Detective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8a359380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "\n",
    "def gather_recipe_data(kaggledataset: str) -> list[str]:\n",
    "    dataset_path = kagglehub.dataset_download(kaggledataset)\n",
    "    df = pd.read_csv(f\"{dataset_path}/Receipes from around the world.csv\", encoding='latin-1')\n",
    "    \n",
    "    # Process the dataframe to list of text entries for retrieval\n",
    "    # Format each recipe as structured text for better retrieval\n",
    "    recipes = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Create a readable text representation of each recipe\n",
    "        recipe_parts = []\n",
    "        for col in df.columns:\n",
    "            value = row[col]\n",
    "            # Skip NaN values and format nicely\n",
    "            if pd.notna(value) and str(value).strip():\n",
    "                recipe_parts.append(f\"{col}: {value}\")\n",
    "        \n",
    "        # Join all parts into a single text entry\n",
    "        recipe_text = \". \".join(recipe_parts)\n",
    "        recipes.append(recipe_text)\n",
    "    \n",
    "    return recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a3b33c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.vectorstores import FAISS\n",
    "# from langchain_core.documents import Document\n",
    "# from langchain.tools import tool\n",
    "\n",
    "# recipes = gather_recipe_data(\"prajwaldongre/collection-of-recipes-around-the-world\")\n",
    "# docs = [Document(page_content=recipe) for recipe in recipes]\n",
    "# vector = FAISS.from_documents(docs, embeddings)\n",
    "# retriever = vector.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d53c0b0",
   "metadata": {},
   "source": [
    "## RAG Tool\n",
    "I created the following:\n",
    "- a folder to store embeddings and faiss index\n",
    "- a rag pipeline file\n",
    "- a file that exposes the rag pipeline as a tool. \n",
    "- I am importing the tool in here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8cafce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from rag_system.rag_pipeline import CulinaryRAG\n",
    "\n",
    "# Load the RAG engine and retriever\n",
    "rag = CulinaryRAG()\n",
    "retriever = rag.load_index()  # returns vectorstore.as_retriever()\n",
    "\n",
    "@tool\n",
    "def retrieve_culinary_context(query: str):\n",
    "    \"\"\"\n",
    "    Retrieves culinary information relevant to country/region origin detection.\n",
    "    Takes a descriptive query (ingredients, cooking method, spices) and performs vector retrieval.\n",
    "    \"\"\"\n",
    "    docs = retriever.invoke(query)\n",
    "    return \"\\n\".join([d.page_content for d in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5d3307f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/sagemaker-user/.cache/kagglehub/datasets/prajwaldongre/collection-of-recipes-around-the-world/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"prajwaldongre/collection-of-recipes-around-the-world\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "44bd8a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def retrieve_recipes(query: str):\n",
    "  \"\"\"\n",
    "  Retrieves recipes based on a search query.\n",
    "  \"\"\"\n",
    "  return retriever.invoke(query)\n",
    "\n",
    "culinary_detective_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"\"\"\n",
    "You are an expert culinary anthropologist. From ingredients and a short description, identify the country and the SPECIFIC region within that country where the dish is most associated.\n",
    "\n",
    "You can consult a retrieval tool bound to this agent (retrieve_culinary_context). Use it when helpful; otherwise reason from your knowledge of ingredients, techniques, and named dishes.\n",
    "\"\"\"},\n",
    "    {\"role\": \"human\", \"content\": \"\"\"\n",
    "Task:\n",
    "Return ONLY a single line in the exact format: [Country], [Region]\n",
    "- Country: full official name (e.g., \"United States\", not \"USA\").\n",
    "- Region: a specific intra-country region (e.g., North, South, East, West, Central, Northeast, etc.).\n",
    "- Use \"All\" only if the dish is truly nationwide.\n",
    "- If no region is identifiable, write \"Unknown\".\n",
    "- Do NOT include any explanation before or after the bracketed answer.\n",
    "\n",
    "Cues to consider:\n",
    "- Ingredients (grains, staple flours, spice blends), cooking methods, named dishes, iconic sides.\n",
    "- Example mappings:\n",
    "  * Brazil: p√£o de queijo / tapioca flour / queijo minas ‚Üí [Brazil], [South]\n",
    "  * Japan: sushi / rice vinegar / tempura ‚Üí [Japan], [All]\n",
    "  * India: dosa / idli / coconut ‚Üí [India], [South]; naan / paneer / tandoor ‚Üí [India], [North]\n",
    "  * Ethiopia: teff / injera / wat / berbere ‚Üí [Ethiopia], [Unknown]\n",
    "  * China: dim sum ‚Üí [China], [South]; hot pot (Sichuan/Chongqing-style) ‚Üí [China], [West]\n",
    "  * Thailand: khao soi ‚Üí [Thailand], [North]; som tam ‚Üí [Thailand], [Northeast]\n",
    "\n",
    "Few-shot examples:\n",
    "Input: \"Fermented teff flatbread served with spicy stews (wat) and berbere.\"\n",
    "Output: [Ethiopia], [Unknown]\n",
    "\n",
    "Input: \"Cheese bread made with tapioca starch, typical with churrasco in the south.\"\n",
    "Output: [Brazil], [South]\n",
    "\n",
    "Input: \"Batter of rice and urad dal, steamed into soft cakes, served with coconut chutney.\"\n",
    "Output: [India], [South]\n",
    "\n",
    "Input: \"Assorted small bites with tea in bamboo steamers, hallmark of Cantonese cuisine.\"\n",
    "Output: [China], [South]\n",
    "\n",
    "Now produce ONLY the answer for the current input as [Country], [Region].\n",
    "\"\"\"},\n",
    "]\n",
    "\n",
    "culinary_detective = create_agent(model=llm, tools=[retrieve_culinary_context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "921d5b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [SystemMessage(content='\\nYou are an expert culinary anthropologist. From ingredients and a short description, identify the country and the SPECIFIC region within that country where the dish is most associated.\\n\\nYou can consult a retrieval tool bound to this agent (retrieve_culinary_context). Use it when helpful; otherwise reason from your knowledge of ingredients, techniques, and named dishes.\\n', additional_kwargs={}, response_metadata={}, id='63b22b05-4c3b-4d66-8b75-3c4383d21380'), HumanMessage(content='\\nTask:\\nReturn ONLY a single line in the exact format: [Country], [Region]\\n- Country: full official name (e.g., \"United States\", not \"USA\").\\n- Region: a specific intra-country region (e.g., North, South, East, West, Central, Northeast, etc.).\\n- Use \"All\" only if the dish is truly nationwide.\\n- If no region is identifiable, write \"Unknown\".\\n- Do NOT include any explanation before or after the bracketed answer.\\n\\nCues to consider:\\n- Ingredients (grains, staple flours, spice blends), cooking methods, named dishes, iconic sides.\\n- Example mappings:\\n  * Brazil: p√£o de queijo / tapioca flour / queijo minas ‚Üí [Brazil], [South]\\n  * Japan: sushi / rice vinegar / tempura ‚Üí [Japan], [All]\\n  * India: dosa / idli / coconut ‚Üí [India], [South]; naan / paneer / tandoor ‚Üí [India], [North]\\n  * Ethiopia: teff / injera / wat / berbere ‚Üí [Ethiopia], [Unknown]\\n  * China: dim sum ‚Üí [China], [South]; hot pot (Sichuan/Chongqing-style) ‚Üí [China], [West]\\n  * Thailand: khao soi ‚Üí [Thailand], [North]; som tam ‚Üí [Thailand], [Northeast]\\n\\nFew-shot examples:\\nInput: \"Fermented teff flatbread served with spicy stews (wat) and berbere.\"\\nOutput: [Ethiopia], [Unknown]\\n\\nInput: \"Cheese bread made with tapioca starch, typical with churrasco in the south.\"\\nOutput: [Brazil], [South]\\n\\nInput: \"Batter of rice and urad dal, steamed into soft cakes, served with coconut chutney.\"\\nOutput: [India], [South]\\n\\nInput: \"Assorted small bites with tea in bamboo steamers, hallmark of Cantonese cuisine.\"\\nOutput: [China], [South]\\n\\nNow produce ONLY the answer for the current input as [Country], [Region].\\n', additional_kwargs={}, response_metadata={}, id='c9afd36f-ded3-4253-844e-1319ec655a69'), HumanMessage(content='The dish is spicy, coconut-based, and served with rice.', additional_kwargs={}, response_metadata={}, id='1e305819-dee1-4e5e-bf22-76d707eb857b'), AIMessage(content='[India], [South]', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1232, 'prompt_tokens': 659, 'total_tokens': 1891, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1216, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CjjCtrcyP28OdLcKbFSKrJNEN3p4s', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--47c193a7-5a8b-46c6-ad04-b44bbf4de8da-0', usage_metadata={'input_tokens': 659, 'output_tokens': 1232, 'total_tokens': 1891, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1216}})]}\n"
     ]
    }
   ],
   "source": [
    "result = culinary_detective.invoke({\n",
    "    \"messages\": culinary_detective_messages + [\n",
    "        {\"role\": \"human\", \"content\": \"The dish is spicy, coconut-based, and served with rice.\"}\n",
    "    ]\n",
    "})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7e6a54a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_culinary_detective_answer(response: str) -> tuple[str, str]:\n",
    "    # TODO: Extract the country and region from the response\n",
    "    # return country, region\n",
    "    import re\n",
    "    \n",
    "    chars = r'\\[([^\\]]+)\\]\\s*,\\s*\\[([^\\]]+)\\]'\n",
    "    match = re.search(chars, response)\n",
    "    \n",
    "    if match:\n",
    "        country = match.group(1).strip()\n",
    "        region = match.group(2).strip()\n",
    "        return country, region\n",
    "    \n",
    "    lines_split = response.strip().split('\\n')\n",
    "    for line in lines_split:\n",
    "        if len(line) > 100:\n",
    "            continue\n",
    "        if ',' in line:\n",
    "            parts = line.split(',')\n",
    "            if len(parts) >= 2:\n",
    "                country = parts[0].strip().strip('[]\"\\'')\n",
    "                region = parts[1].strip().strip('[]\"\\'')\n",
    "                for prefix in ['The answer is', 'Answer:', 'Country:', 'Region:']:\n",
    "                    country = country.replace(prefix, '').strip()\n",
    "                    region = region.replace(prefix, '').strip()\n",
    "                return country, region\n",
    "    \n",
    "    return \"\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e9400a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_culinary_detective_answer(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86672811",
   "metadata": {},
   "source": [
    "### üëÑLingua Locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0ad20b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "lingua_locale_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"\"\"\n",
    "You are an expert in languages, scripts, orthography, and regional vocabulary. Determine which country's website or text a sentence most likely comes from.\n",
    "\"\"\"},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "You must ONLY return a single line: [Country]. You should use the full official country name (e.g., \"United States\", not \"USA\"). \n",
    "- You should not add any other explanations. Only output the name of the country.\n",
    "\n",
    "Here are some heuristics:\n",
    "- English: colour, organise, centre ‚Üí [United Kingdom]; sidewalk, color, organize ‚Üí [United States].\n",
    "- Portuguese: autocarro, factura, telem√≥vel ‚Üí [Portugal]; √¥nibus, nota fiscal, celular ‚Üí [Brazil].\n",
    "- Spanish: vos, colectivo, pileta ‚Üí [Argentina]; ordenador ‚Üí [Spain]; autob√∫s ‚Üí [Mexico/Spain], coche (Spain) vs carro (LatAm).\n",
    "- Chinese: Traditional characters (Ëá∫ÁÅ£„ÄÅËá∫Âåó„ÄÅÁπÅÈ´î) ‚Üí [Taiwan]; Simplified (‰∏≠ÂõΩ„ÄÅÂπøÂ∑û„ÄÅÁÆÄ‰Ωì) ‚Üí [China].\n",
    "- Cyrillic specifics: –¥—ò / —í / —õ / —ô / —ö ‚Üí [Montenegro]; –¥–µ–≤–æ—ò–∫–∞ / —í–∞–∫ more typical of [Serbia].\n",
    "- French: anglicisms + CAD context ‚Üí [Canada]; m√©tropolitain cues ‚Üí [France].\n",
    "- Haitian Creole ‚Üí [Haiti].\n",
    "\n",
    "Examples:\n",
    "Input: \"Please, colour is the preferred spelling in our centre.\"\n",
    "Output: [United Kingdom]\n",
    "\n",
    "Input: \"Clique para imprimir a fatura no seu telem√≥vel.\"\n",
    "Output: [Portugal]\n",
    "\n",
    "Input: \"–°–∏–Ω–æ—õ —Å–∞–º –≤–∏–¥–∏–æ –¥—ò–µ–≤–æ—ò–∫—É —É –ü–æ–¥–≥–æ—Ä–∏—Ü–∏.\"\n",
    "Output: [Montenegro]\n",
    "\n",
    "Input: \"ÈÄôÊòØËá∫ÁÅ£Êú¨Âú∞ÁöÑÈ†ÅÈù¢„ÄÇ\"\n",
    "Output: [Taiwan]\n",
    "\n",
    "Now produce ONLY [Country] for the current sentence.\n",
    "\"\"\"},\n",
    "]\n",
    "\n",
    "lingua_locale = create_agent(model=llm, tools=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5da2a766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lingua_locale_answer(response: str) -> tuple[str, str]:\n",
    "    # TODO: Extract the country and \"none\" from the response\n",
    "    # only the first field is used, the second is a dummy field to make the return type consistent\n",
    "    # return country, \"none\"\n",
    "    import re\n",
    "    \n",
    "    chars = r'\\[([^\\]]+)\\]'\n",
    "    match = re.search(chars, response)\n",
    "    \n",
    "    if match:\n",
    "        country = match.group(1).strip()\n",
    "        return country, \"none\"\n",
    "    \n",
    "    lines_split = response.strip().split('\\n')\n",
    "    for line in lines_split:\n",
    "        if len(line) > 100:\n",
    "            continue\n",
    "        clean_line = line.strip().strip('[]\"\\'')\n",
    "        for prefix in ['The answer is', 'Answer:', 'Country:', 'The country is', 'This is from']:\n",
    "            clean_line = clean_line.replace(prefix, '').strip()\n",
    "        \n",
    "        if clean_line and len(clean_line) < 50:\n",
    "            clean_line = clean_line.rstrip('.,;:')\n",
    "            return clean_line, \"none\"\n",
    "    \n",
    "    return \"\", \"none\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e3291f",
   "metadata": {},
   "source": [
    "## Answering questions\n",
    "This part includes how we load the questions and generate the prediction in desired format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "59f9861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geoguesser(q: dict, print_raw_response=False) -> tuple[str, str]:\n",
    "    if q[\"type\"] == \"GlobalTrekker\":\n",
    "        query = {\"role\": \"user\", \"content\": f\"Paragraph: {q['paragraph']}\"}\n",
    "        messages, agent, extractor = global_trekker_messages, global_trekker, extract_global_trekker_answer\n",
    "    elif q[\"type\"] == \"CulinaryDetective\":\n",
    "        query = {\"role\": \"user\", \"content\": f\"Ingredients: {q['ingredient']}. Description: {q['description']}\"}\n",
    "        messages, agent, extractor = culinary_detective_messages, culinary_detective, extract_culinary_detective_answer\n",
    "    else: #q[\"type\"] == \"LinguaLocale\":\n",
    "        query = {\"role\": \"user\", \"content\": f\"Sentence: {q['sentence']}\"}\n",
    "        messages, agent, extractor = lingua_locale_messages, lingua_locale, extract_lingua_locale_answer\n",
    "\n",
    "    response_all = agent.invoke({\"messages\": messages + [query]})\n",
    "    response = response_all[\"messages\"][-1].content\n",
    "    if print_raw_response: print(f\"{q['type']}: {response_all}\")\n",
    "    return extractor(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "45950e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Here, we load the examples questions. Public/private set will be in the same format\n",
    "dataset_name = \"public.jsonl\"\n",
    "questions = []\n",
    "with open(dataset_name, \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        questions.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "14091132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GlobalTrekker: {'messages': [SystemMessage(content='You are an expert in world knowledge.', additional_kwargs={}, response_metadata={}, id='920f974d-322d-4477-8d17-b5e4921630d6'), HumanMessage(content='Given the following paragraph, guess the most likely country and city. Answer in the format of [country], [city]. If the paragraph is generic or only country-level, return \"Unknown\" for city.', additional_kwargs={}, response_metadata={}, id='2d6bb0e2-5419-430c-a521-92b1284f829c'), HumanMessage(content='Paragraph: Uluru, also known as Ayers Rock, is a large sandstone monolith that is sacred to the Pitjantjatjara, the Aboriginal people of the area, known as the A·πâangu. It is one of the most important indigenous sites of the country, being a popular destination for tourists since the 1930s. The area around the formation is home to many springs, waterholes, rock caves and ancient paintings. Uluru is listed as a UNESCO World Heritage Site.', additional_kwargs={}, response_metadata={}, id='68d521a5-0166-4562-a877-f20c726340de'), AIMessage(content='Australia, Yulara', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 527, 'prompt_tokens': 163, 'total_tokens': 690, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 512, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CjjD6PXQy5d500q6TXEtL9tf6B4q5', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--76fb9a89-dc28-461a-b941-db51d6115bc1-0', usage_metadata={'input_tokens': 163, 'output_tokens': 527, 'total_tokens': 690, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 512}})]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Australia', 'Yulara')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test run on one question\n",
    "# You might want to save the raw response for debugging answer formatting/extraction\n",
    "# If the extracted answer seems off, check the raw response instead of running inference repeatedly\n",
    "geoguesser(questions[0], print_raw_response=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "339601c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 144/170 [15:32<05:12, 12.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing question {'type': 'LinguaLocale', 'sentence': 'ÏßÄÎÇúÏ£º Í≤ΩÍ∏∞ÎèÑ Ïù∏Ï≤úÏóêÏÑú Ï∞®Îüâ Ïö¥Ï†ÑÏûêÍ∞Ä Í∞ÄÏÜç ÌéòÎã¨ÏùÑ ÏûòÎ™ª Î∞üÏïÑ Í∏∏ÏùÑ ÏßÄÎÇòÎçò Î≥¥ÌñâÏûêÎ•º ÏπòÏñ¥ Ïà®ÏßÄÍ≤å Ìïú ÌòêÏùòÎ°ú Í≤ΩÏ∞∞ ÏàòÏÇ¨Î•º Î∞õÍ≥† ÏûàÎäî Í≤ÉÏúºÎ°ú ÌååÏïÖÎêêÏäµÎãàÎã§.', 'country': 'Korea, South', 'info': {'reference': 'https://imnews.imbc.com/news/2025/society/article/6776876_36718.html', 'hints': 'orthography, \"Í≤ΩÍ∏∞ÎèÑ Ïù∏Ï≤ú\" (Gyeonggi-do, Incheon)'}}: Error code: 400 - {'error': {'message': \"litellm.BadRequestError: litellm.ContentPolicyViolationError: litellm.ContentPolicyViolationError: AzureException - The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\\nmodel=gpt-5. content_policy_fallback=None. fallbacks=None.\\n\\nSet 'content_policy_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=gpt-5\\nAvailable Model Group Fallbacks=None\", 'type': None, 'param': None, 'code': '400'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [19:03<00:00,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to public.txt (private set)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample script to generate answers\n",
    "from tqdm import tqdm\n",
    "answers = []\n",
    "for q in tqdm(questions):\n",
    "    try:\n",
    "        country, category = geoguesser(q)\n",
    "        answers.append(f\"{q['type']}\\t{country}\\t{category}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question {q}: {e}\")\n",
    "        answers.append(f\"{q['type']}\\tUnknown\\tUnknown\")\n",
    "\n",
    "with open(\"public.txt\", \"w\") as f:\n",
    "    for answer in answers:\n",
    "        f.write(answer + \"\\n\")\n",
    "\n",
    "print(\"Saved predictions to public.txt (private set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99774c89",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "This is how we calculate the scores on Gradescope (details subject to change, but the general logic will stay the same)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e478646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_match(answer, expectedAnswer):\n",
    "    score = 0.0\n",
    "    if expectedAnswer in answer:\n",
    "        score = len(expectedAnswer) / len(answer)\n",
    "    return score\n",
    "\n",
    "def exact_match(answer, expectedAnswer):\n",
    "    score = 0.0\n",
    "    if expectedAnswer == answer:\n",
    "        score = 1.0\n",
    "    return score    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f16b03ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GlobalTrekker Average Score: 0.8684\n",
      "CulinaryDetective Average Score: 0.5455\n",
      "LinguaLocale Average Score: 0.9140\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "for q in questions:\n",
    "    answers.append((q[\"type\"], q[\"country\"], q.get(\"city\", q.get(\"region\", \"\"))))\n",
    "with open(\"public.txt\", \"r\") as f:\n",
    "    preds = [line.split(\"\\t\") for line in f.readlines()]\n",
    "\n",
    "scores = {\"GlobalTrekker\": [], \"CulinaryDetective\": [], \"LinguaLocale\": []}\n",
    "for (q_type, exp_country, exp_place), (p_type, pred_country, pred_place) in zip(answers, preds):\n",
    "    assert q_type == p_type\n",
    "    country_score = soft_match(pred_country, exp_country)\n",
    "    category_score = 0.0\n",
    "    weights = [0.0, 0.0]\n",
    "    if q_type == \"GlobalTrekker\":\n",
    "        #  correct country -> 80%, correct country and city -> +20%\n",
    "        weights = [0.8, 0.2]\n",
    "        if country_score > 0:\n",
    "            if exp_place == \"None\": category_score = 1.0\n",
    "            else: category_score = soft_match(pred_place, exp_place)\n",
    "    elif q_type == \"CulinaryDetective\":\n",
    "        # correct country -> 60%, correct country and region -> +40%\n",
    "        weights = [0.6, 0.4]\n",
    "        if country_score > 0:\n",
    "            if exp_place == \"None\": category_score = 1.0\n",
    "            else: category_score = exact_match(pred_place, exp_place)\n",
    "    else: # LinguaLocale\n",
    "        # correct country -> 60%, matched official language -> +40%\n",
    "        weights = [0.6, 0.4]\n",
    "        if country_score > 0:\n",
    "            category_score = 1.0\n",
    "        else: # incorrect country. language match works only if pred_country is a clean answer\n",
    "            exp_langs = list_of_countries.get(exp_country, [])\n",
    "            pred_langs = list_of_countries.get(pred_country, [])\n",
    "            if any(lang in exp_langs for lang in pred_langs):\n",
    "                category_score = 1.0\n",
    "\n",
    "    score = weights[0] * country_score + weights[1] * category_score\n",
    "    scores[q_type].append(score)\n",
    "\n",
    "for q_type, score_list in scores.items():\n",
    "    avg_score = sum(score_list) / len(score_list)\n",
    "    print(f\"{q_type} Average Score: {avg_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "603e38d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(num_exact: int, num_total: int) -> float:\n",
    "    acc_score = 0.0\n",
    "    if num_total > 0:\n",
    "        score = (num_exact / num_total)*100\n",
    "    return score\n",
    "\n",
    "# Let's calculate Precision, Recall, F1\n",
    "def calculate_metrics(tp, fp, fn):\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "65d74ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GlobalTrekker Accuracy (% exact): 60.6557 | exact=37 / total=61\n",
      "CulinaryDetective Accuracy (% exact): 43.6364 | exact=24 / total=55\n",
      "LinguaLocale Accuracy (% exact): 87.0370 | exact=47 / total=54\n"
     ]
    }
   ],
   "source": [
    "# Per-type exact-match accuracy (robust)\n",
    "from collections import Counter\n",
    "\n",
    "# 1) Build expected answers per question with safe defaults\n",
    "exp = []\n",
    "for q in questions:\n",
    "    q_type = q.get(\"type\", \"Unknown\")\n",
    "    exp_country = q.get(\"country\", \"Unknown\")\n",
    "    if q_type == \"GlobalTrekker\":\n",
    "        exp_place = q.get(\"city\", \"Unknown\")\n",
    "    elif q_type == \"CulinaryDetective\":\n",
    "        exp_place = q.get(\"region\", \"Unknown\")\n",
    "    else:\n",
    "        exp_place = \"\"\n",
    "    exp.append((q_type, exp_country, exp_place))\n",
    "\n",
    "# 2) Read predictions and pad/truncate to 3 fields\n",
    "preds_raw = []\n",
    "with open(\"public.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        parts = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        parts = (parts + [\"Unknown\", \"Unknown\"])[:3]\n",
    "        preds_raw.append(parts)\n",
    "\n",
    "# 3) Count totals per type\n",
    "type_counts = Counter([t for t,_,_ in exp])\n",
    "\n",
    "# 4) Exact-match counters per type\n",
    "exact_counts = {\"GlobalTrekker\": 0, \"CulinaryDetective\": 0, \"LinguaLocale\": 0}\n",
    "\n",
    "# 5) Iterate and count exact matches according to task rules\n",
    "for (q_type, exp_country, exp_place), (p_type, pred_country, pred_place) in zip(exp, preds_raw):\n",
    "    if q_type != p_type:\n",
    "        # Skip mismatches to avoid misaligned files\n",
    "        continue\n",
    "    if q_type == \"GlobalTrekker\":\n",
    "        if exact_match(pred_country, exp_country) == 1.0 and exact_match(pred_place, exp_place) == 1.0:\n",
    "            exact_counts[q_type] += 1\n",
    "    elif q_type == \"CulinaryDetective\":\n",
    "        if exact_match(pred_country, exp_country) == 1.0 and exact_match(pred_place, exp_place) == 1.0:\n",
    "            exact_counts[q_type] += 1\n",
    "    else:  # LinguaLocale (country only)\n",
    "        if exact_match(pred_country, exp_country) == 1.0:\n",
    "            exact_counts[q_type] += 1\n",
    "\n",
    "# 6) Compute accuracy (%) per type\n",
    "def _acc(num_exact: int, num_total: int) -> float:\n",
    "    return (num_exact / num_total) * 100 if num_total > 0 else 0.0\n",
    "\n",
    "for t in [\"GlobalTrekker\", \"CulinaryDetective\", \"LinguaLocale\"]:\n",
    "    total = type_counts.get(t, 0)\n",
    "    acc = _acc(exact_counts.get(t, 0), total)\n",
    "    print(f\"{t} Accuracy (% exact): {acc:.4f} | exact={exact_counts.get(t,0)} / total={total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9e3c3b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GlobalTrekker:  P=1.0000  R=0.6066  F1=0.7551   (TP=37, FP=0, FN=24, Total positives=61)\n",
      "CulinaryDetective:  P=1.0000  R=0.4364  F1=0.6076   (TP=24, FP=0, FN=31, Total positives=55)\n",
      "LinguaLocale:  P=1.0000  R=0.8704  F1=0.9307   (TP=47, FP=0, FN=7, Total positives=54)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def calculate_metrics(tp, fp, fn):\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Containers\n",
    "tp = defaultdict(int)\n",
    "fp = defaultdict(int)\n",
    "fn = defaultdict(int)\n",
    "counts = defaultdict(int)\n",
    "\n",
    "task_types = [\"GlobalTrekker\", \"CulinaryDetective\", \"LinguaLocale\"]\n",
    "\n",
    "for (q_type, exp_country, exp_place), (p_type, pred_country, pred_place) in zip(exp, preds_raw):\n",
    "\n",
    "    for T in task_types:\n",
    "\n",
    "        # Ground truth condition\n",
    "        gt_positive = (q_type == T)\n",
    "        # Predicted condition\n",
    "        pred_positive = (p_type == T)\n",
    "\n",
    "        # Define exact match under that type\n",
    "        if T == \"GlobalTrekker\":\n",
    "            exact = (pred_country == exp_country) and (pred_place == exp_place)\n",
    "        elif T == \"CulinaryDetective\":\n",
    "            exact = (pred_country == exp_country) and (pred_place == exp_place)\n",
    "        else:  # LinguaLocale\n",
    "            exact = (pred_country == exp_country)\n",
    "\n",
    "        if gt_positive:\n",
    "            counts[T] += 1\n",
    "\n",
    "        # Evaluate classification outcomes\n",
    "        if gt_positive and pred_positive:\n",
    "            # Either TP or FN\n",
    "            if exact:\n",
    "                tp[T] += 1\n",
    "            else:\n",
    "                fn[T] += 1\n",
    "        elif (not gt_positive) and pred_positive:\n",
    "            # FP\n",
    "            fp[T] += 1\n",
    "        elif gt_positive and (not pred_positive):\n",
    "            # FN ‚Äî model failed to predict this type\n",
    "            fn[T] += 1\n",
    "        # TN is ignored\n",
    "\n",
    "# Print results\n",
    "for T in task_types:\n",
    "    precision, recall, f1 = calculate_metrics(tp[T], fp[T], fn[T])\n",
    "    print(f\"{T}:  P={precision:.4f}  R={recall:.4f}  F1={f1:.4f}   \"\n",
    "          f\"(TP={tp[T]}, FP={fp[T]}, FN={fn[T]}, Total positives={counts[T]})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Universal Environment",
   "language": "python",
   "name": "ml-universal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
