{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f3fe1f",
   "metadata": {},
   "source": [
    "# HW4: QA Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c91ec1",
   "metadata": {},
   "source": [
    "## Dependencies and LLM Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42689839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain==1.0.5\n",
    "# !pip install langchain-core\n",
    "# !pip install langchain-community\n",
    "# !pip install faiss-cpu\n",
    "# !pip install kagglehub\n",
    "device = \"cuda\"  # \"cpu\" or \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7af723c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the list of countries we are using (with their official languages)\n",
    "# Feel free to use it in your code\n",
    "list_of_countries = {}\n",
    "with open(\"countries_with_languages.tsv\", \"r\"  ) as f:\n",
    "    for line in f.readlines():\n",
    "        country, langs = line.strip().split(\"\\t\")\n",
    "        list_of_countries[country] = langs.split(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c06a7d",
   "metadata": {},
   "source": [
    "### Choice 1: OpenAI API\n",
    "\n",
    "The notebook's implementation is based on this.\n",
    "Feel free to change the model, and please keep track of your usage on the \"Usage\" page on [LiteLLM API webpage](https://ai-gateway.andrew.cmu.edu/ui/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b65e4808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5add67b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "import getpass, os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
    "openai_model_id = \"gpt-5\"\n",
    "openai_embmodel_id = \"azure/text-embedding-3-small\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=openai_model_id,\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=\"https://ai-gateway.andrew.cmu.edu/\"\n",
    ")\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=openai_embmodel_id,\n",
    "    api_key=os.environ['OPENAI_API_KEY'],\n",
    "    base_url='https://ai-gateway.andrew.cmu.edu/'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799b00ca",
   "metadata": {},
   "source": [
    "### Choice 2: Hugging Face Models\n",
    "\n",
    "You may also use Hugging Face models without API credits if you have available GPU resource. You might have to the change prompt templates according to your model choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac1bc014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-huggingface text-generation transformers google-search-results \n",
    "# !pip install numexpr langchainhub sentencepiece sentence-transformers jinja2 bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbf0fa7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 08:36:23.968740: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764750983.997946   63645 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764750984.010012   63645 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-12-03 08:36:24.258471: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import getpass, os, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline, HuggingFaceEmbeddings\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass(\"Enter your Hugging Face API key: \")\n",
    "hgf_model_id = \"Qwen/Qwen3-0.6B\"\n",
    "hgf_embmodel_id = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# Select device and dtype\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "\n",
    "# Build HF generation pipeline on GPU when available\n",
    "tokenizer = AutoTokenizer.from_pretrained(hgf_model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    hgf_model_id,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "gen_pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=dtype,\n",
    ")\n",
    "\n",
    "hgf_model = HuggingFacePipeline(pipeline=gen_pipe)\n",
    "hgf_llm = ChatHuggingFace(llm=hgf_model)\n",
    "\n",
    "# Place sentence-transformers embeddings on GPU when available\n",
    "hgf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=hgf_embmodel_id,\n",
    "    model_kwargs={\"device\": device}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a37cbe",
   "metadata": {},
   "source": [
    "## Handling different type of questions\n",
    "\n",
    "Implement the answer formatting and extraction for each question type. You may change the prompt to fit your processing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e242dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6a4262",
   "metadata": {},
   "source": [
    "### ðŸ—ºï¸Global Trekker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bce1dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_trekker_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert in world geography and cultural geography. Given a descriptive paragraph, infer the most likely country and city with high precision.\"},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "Read the paragraph and extract:\n",
    "- Country: use the full official country name (e.g., \"United States\", not \"USA\").\n",
    "- City: a specific city if clearly indicated by clues (landmarks, neighborhoods, transit lines, local foods, dialects). If not identifiable, write \"Unknown\".\n",
    "\n",
    "Strict output format (no explanations):\n",
    "[Country], [City]\n",
    "\n",
    "Guidelines:\n",
    "- Prefer unique cues (local transit names, street names, postal formats, phone codes, currency, cuisine, sports clubs).\n",
    "- If multiple cities fit, pick the single most likely one.\n",
    "- If the paragraph is generic or only country-level, return \"Unknown\" for city.\n",
    "- Do not include any extra text besides the bracketed pair.\n",
    "\n",
    "Examples:\n",
    "Input: \"We walked along the Arno past the Ponte Vecchio before climbing to Piazzale Michelangelo.\"\n",
    "Output: [Italy], [Florence]\n",
    "\n",
    "Input: \"A red double-decker bus passed by the Thames near Westminster Abbey and Big Ben.\"\n",
    "Output: [United Kingdom], [London]\n",
    "\n",
    "Input: \"We grabbed Primanti's near the confluence of the Allegheny and Monongahela.\"\n",
    "Output: [United States], [Pittsburgh]\n",
    "\n",
    "Input: \"I toured the Old Town and the Royal Mile before seeing the castle on the hill.\"\n",
    "Output: [United Kingdom], [Edinburgh]\n",
    "\n",
    "Input: \"We enjoyed maple syrup and poutine while skating on a canal in winter.\"\n",
    "Output: [Canada], [Ottawa]\n",
    "\"\"\"},\n",
    "]\n",
    "\n",
    "global_trekker = create_agent(model=llm, tools=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85342fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_global_trekker_answer(response: str) -> tuple[str, str]:\n",
    "    # TODO: Extract the country and city from the response\n",
    "    # return country, city\n",
    "    import re\n",
    "    \n",
    "    # Look for pattern [country], [city] or variations\n",
    "    # Try to find content within brackets first\n",
    "    bracket_pattern = r'\\[([^\\]]+)\\]\\s*,\\s*\\[([^\\]]+)\\]'\n",
    "    match = re.search(bracket_pattern, response)\n",
    "    \n",
    "    if match:\n",
    "        country = match.group(1).strip()\n",
    "        city = match.group(2).strip()\n",
    "        return country, city\n",
    "    \n",
    "    # Fallback: look for two items separated by comma\n",
    "    # Often LLMs will say something like \"United States, Pittsburgh\"\n",
    "    lines = response.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        # Skip lines that are too long (likely explanations)\n",
    "        if len(line) > 100:\n",
    "            continue\n",
    "        # Look for comma-separated values\n",
    "        if ',' in line:\n",
    "            parts = line.split(',')\n",
    "            if len(parts) >= 2:\n",
    "                country = parts[0].strip().strip('[]\"\\'')\n",
    "                city = parts[1].strip().strip('[]\"\\'')\n",
    "                # Clean up common prefixes\n",
    "                for prefix in ['The answer is', 'Answer:', 'Location:', 'Country:', 'City:']:\n",
    "                    country = country.replace(prefix, '').strip()\n",
    "                    city = city.replace(prefix, '').strip()\n",
    "                return country, city\n",
    "    \n",
    "    # Last resort: return empty strings\n",
    "    return \"\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd5210d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('AAA', 'BBB')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test run your extration function before using it in the main loop!\n",
    "extract_global_trekker_answer(\"AAA, BBB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78ab9c1",
   "metadata": {},
   "source": [
    "### ðŸ½ï¸Culinary Detective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a359380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "\n",
    "def gather_recipe_data(kaggledataset: str) -> list[str]:\n",
    "    dataset_path = kagglehub.dataset_download(kaggledataset)\n",
    "    df = pd.read_csv(f\"{dataset_path}/Receipes from around the world.csv\", encoding='latin-1')\n",
    "    \n",
    "    # Process the dataframe to list of text entries for retrieval\n",
    "    # Format each recipe as structured text for better retrieval\n",
    "    recipes = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Create a readable text representation of each recipe\n",
    "        recipe_parts = []\n",
    "        for col in df.columns:\n",
    "            value = row[col]\n",
    "            # Skip NaN values and format nicely\n",
    "            if pd.notna(value) and str(value).strip():\n",
    "                recipe_parts.append(f\"{col}: {value}\")\n",
    "        \n",
    "        # Join all parts into a single text entry\n",
    "        recipe_text = \". \".join(recipe_parts)\n",
    "        recipes.append(recipe_text)\n",
    "    \n",
    "    return recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3b33c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain.tools import tool\n",
    "\n",
    "recipes = gather_recipe_data(\"prajwaldongre/collection-of-recipes-around-the-world\")\n",
    "docs = [Document(page_content=recipe) for recipe in recipes]\n",
    "vector = FAISS.from_documents(docs, embeddings)\n",
    "retriever = vector.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d53c0b0",
   "metadata": {},
   "source": [
    "## RAG Tool\n",
    "I created the following:\n",
    "- a folder to store embeddings and faiss index\n",
    "- a rag pipeline file\n",
    "- a file that exposes the rag pipeline as a tool. \n",
    "- I am importing the tool in here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cafce3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/codespace/nlp_assignments/qa_agent/rag_system/rag_pipeline.py:13: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "from rag_system.rag_pipeline import CulinaryRAG\n",
    "\n",
    "# Load the RAG engine and retriever\n",
    "rag = CulinaryRAG()\n",
    "retriever = rag.load_index()  # returns vectorstore.as_retriever()\n",
    "\n",
    "@tool\n",
    "def retrieve_culinary_context(query: str):\n",
    "    \"\"\"\n",
    "    Retrieves culinary information relevant to country/region origin detection.\n",
    "    Takes a descriptive query (ingredients, cooking method, spices) and performs vector retrieval.\n",
    "    \"\"\"\n",
    "    docs = retriever.invoke(query)\n",
    "    return \"\\n\".join([d.page_content for d in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44bd8a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def retrieve_recipes(query: str):\n",
    "  \"\"\"\n",
    "  Retrieves recipes based on a search query.\n",
    "  \"\"\"\n",
    "  return retriever.invoke(query)\n",
    "\n",
    "culinary_detective_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"\"\"\n",
    "You are an expert culinary anthropologist. From ingredients and a short description, identify the country and the SPECIFIC region within that country where the dish is most associated.\n",
    "\n",
    "You can consult a retrieval tool bound to this agent (retrieve_culinary_context). Use it when helpful; otherwise reason from your knowledge of ingredients, techniques, and named dishes.\n",
    "\"\"\"},\n",
    "    {\"role\": \"human\", \"content\": \"\"\"\n",
    "Task:\n",
    "Return ONLY a single line in the exact format: [Country], [Region]\n",
    "- Country: full official name (e.g., \"United States\", not \"USA\").\n",
    "- Region: a specific intra-country region (e.g., North, South, East, West, Central, Northeast, etc.).\n",
    "- Use \"All\" only if the dish is truly nationwide.\n",
    "- If no region is identifiable, write \"Unknown\".\n",
    "- Do NOT include any explanation before or after the bracketed answer.\n",
    "\n",
    "Cues to consider:\n",
    "- Ingredients (grains, staple flours, spice blends), cooking methods, named dishes, iconic sides.\n",
    "- Example mappings:\n",
    "  * Brazil: pÃ£o de queijo / tapioca flour / queijo minas â†’ [Brazil], [South]\n",
    "  * Japan: sushi / rice vinegar / tempura â†’ [Japan], [All]\n",
    "  * India: dosa / idli / coconut â†’ [India], [South]; naan / paneer / tandoor â†’ [India], [North]\n",
    "  * Ethiopia: teff / injera / wat / berbere â†’ [Ethiopia], [Unknown]\n",
    "  * China: dim sum â†’ [China], [South]; hot pot (Sichuan/Chongqing-style) â†’ [China], [West]\n",
    "  * Thailand: khao soi â†’ [Thailand], [North]; som tam â†’ [Thailand], [Northeast]\n",
    "\n",
    "Few-shot examples:\n",
    "Input: \"Fermented teff flatbread served with spicy stews (wat) and berbere.\"\n",
    "Output: [Ethiopia], [Unknown]\n",
    "\n",
    "Input: \"Cheese bread made with tapioca starch, typical with churrasco in the south.\"\n",
    "Output: [Brazil], [South]\n",
    "\n",
    "Input: \"Batter of rice and urad dal, steamed into soft cakes, served with coconut chutney.\"\n",
    "Output: [India], [South]\n",
    "\n",
    "Input: \"Assorted small bites with tea in bamboo steamers, hallmark of Cantonese cuisine.\"\n",
    "Output: [China], [South]\n",
    "\n",
    "Now produce ONLY the answer for the current input as [Country], [Region].\n",
    "\"\"\"},\n",
    "]\n",
    "\n",
    "culinary_detective = create_agent(model=llm, tools=[retrieve_culinary_context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "921d5b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [SystemMessage(content='\\nYou are an expert culinary anthropologist. From ingredients and a short description, identify the country and the SPECIFIC region within that country where the dish is most associated.\\n\\nYou can consult a retrieval tool bound to this agent (retrieve_culinary_context). Use it when helpful; otherwise reason from your knowledge of ingredients, techniques, and named dishes.\\n', additional_kwargs={}, response_metadata={}, id='ef15957c-ed1e-43dd-b98d-50270cde169a'), HumanMessage(content='\\nTask:\\nReturn ONLY a single line in the exact format: [Country], [Region]\\n- Country: full official name (e.g., \"United States\", not \"USA\").\\n- Region: a specific intra-country region (e.g., North, South, East, West, Central, Northeast, etc.).\\n- Use \"All\" only if the dish is truly nationwide.\\n- If no region is identifiable, write \"Unknown\".\\n- Do NOT include any explanation before or after the bracketed answer.\\n\\nCues to consider:\\n- Ingredients (grains, staple flours, spice blends), cooking methods, named dishes, iconic sides.\\n- Example mappings:\\n  * Brazil: pÃ£o de queijo / tapioca flour / queijo minas â†’ [Brazil], [South]\\n  * Japan: sushi / rice vinegar / tempura â†’ [Japan], [All]\\n  * India: dosa / idli / coconut â†’ [India], [South]; naan / paneer / tandoor â†’ [India], [North]\\n  * Ethiopia: teff / injera / wat / berbere â†’ [Ethiopia], [Unknown]\\n  * China: dim sum â†’ [China], [South]; hot pot (Sichuan/Chongqing-style) â†’ [China], [West]\\n  * Thailand: khao soi â†’ [Thailand], [North]; som tam â†’ [Thailand], [Northeast]\\n\\nFew-shot examples:\\nInput: \"Fermented teff flatbread served with spicy stews (wat) and berbere.\"\\nOutput: [Ethiopia], [Unknown]\\n\\nInput: \"Cheese bread made with tapioca starch, typical with churrasco in the south.\"\\nOutput: [Brazil], [South]\\n\\nInput: \"Batter of rice and urad dal, steamed into soft cakes, served with coconut chutney.\"\\nOutput: [India], [South]\\n\\nInput: \"Assorted small bites with tea in bamboo steamers, hallmark of Cantonese cuisine.\"\\nOutput: [China], [South]\\n\\nNow produce ONLY the answer for the current input as [Country], [Region].\\n', additional_kwargs={}, response_metadata={}, id='56d6740f-0921-49be-93c4-35da6d160ada'), HumanMessage(content='The dish is spicy, coconut-based, and served with rice.', additional_kwargs={}, response_metadata={}, id='d3d0ea34-4dde-47bf-aae8-5158bf6e0383'), AIMessage(content='[India], [South]', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 720, 'prompt_tokens': 659, 'total_tokens': 1379, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 704, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CiceCuCqBQneaXECakdYnLFvQeLU1', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--1880846d-03e6-41a2-a12c-316974c70503-0', usage_metadata={'input_tokens': 659, 'output_tokens': 720, 'total_tokens': 1379, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 704}})]}\n"
     ]
    }
   ],
   "source": [
    "result = culinary_detective.invoke({\n",
    "    \"messages\": culinary_detective_messages + [\n",
    "        {\"role\": \"human\", \"content\": \"The dish is spicy, coconut-based, and served with rice.\"}\n",
    "    ]\n",
    "})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e6a54a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_culinary_detective_answer(response: str) -> tuple[str, str]:\n",
    "    # TODO: Extract the country and region from the response\n",
    "    # return country, region\n",
    "    import re\n",
    "    \n",
    "    # Look for pattern [country], [region] or variations\n",
    "    bracket_pattern = r'\\[([^\\]]+)\\]\\s*,\\s*\\[([^\\]]+)\\]'\n",
    "    match = re.search(bracket_pattern, response)\n",
    "    \n",
    "    if match:\n",
    "        country = match.group(1).strip()\n",
    "        region = match.group(2).strip()\n",
    "        return country, region\n",
    "    \n",
    "    # Fallback: look for two items separated by comma\n",
    "    lines = response.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        # Skip lines that are too long (likely explanations)\n",
    "        if len(line) > 100:\n",
    "            continue\n",
    "        # Look for comma-separated values\n",
    "        if ',' in line:\n",
    "            parts = line.split(',')\n",
    "            if len(parts) >= 2:\n",
    "                country = parts[0].strip().strip('[]\"\\'')\n",
    "                region = parts[1].strip().strip('[]\"\\'')\n",
    "                # Clean up common prefixes\n",
    "                for prefix in ['The answer is', 'Answer:', 'Country:', 'Region:']:\n",
    "                    country = country.replace(prefix, '').strip()\n",
    "                    region = region.replace(prefix, '').strip()\n",
    "                return country, region\n",
    "    \n",
    "    # Last resort: return empty strings\n",
    "    return \"\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9400a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_culinary_detective_answer(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86672811",
   "metadata": {},
   "source": [
    "### ðŸ‘„Lingua Locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ad20b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "lingua_locale_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"\"\"\n",
    "You are an expert in languages, scripts, orthography, and regional vocabulary. Determine which country's website or text a sentence most likely comes from.\n",
    "\"\"\"},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "Return ONLY a single line: [Country]\n",
    "- Use the full official country name (e.g., \"United States\", not \"USA\").\n",
    "- No explanations. Do not output anything else.\n",
    "\n",
    "Heuristics:\n",
    "- English: colour, organise, centre â†’ [United Kingdom]; sidewalk, color, organize â†’ [United States].\n",
    "- Portuguese: autocarro, factura, telemÃ³vel â†’ [Portugal]; Ã´nibus, nota fiscal, celular â†’ [Brazil].\n",
    "- Spanish: vos, colectivo, pileta â†’ [Argentina]; ordenador â†’ [Spain]; autobÃºs â†’ [Mexico/Spain], coche (Spain) vs carro (LatAm).\n",
    "- Chinese: Traditional characters (è‡ºç£ã€è‡ºåŒ—ã€ç¹é«”) â†’ [Taiwan]; Simplified (ä¸­å›½ã€å¹¿å·žã€ç®€ä½“) â†’ [China].\n",
    "- Cyrillic specifics: Ð´Ñ˜ / Ñ’ / Ñ› / Ñ™ / Ñš â†’ [Montenegro]; Ð´ÐµÐ²Ð¾Ñ˜ÐºÐ° / Ñ’Ð°Ðº more typical of [Serbia].\n",
    "- French: anglicisms + CAD context â†’ [Canada]; mÃ©tropolitain cues â†’ [France].\n",
    "- Haitian Creole â†’ [Haiti].\n",
    "\n",
    "Examples:\n",
    "Input: \"Please, colour is the preferred spelling in our centre.\"\n",
    "Output: [United Kingdom]\n",
    "\n",
    "Input: \"Clique para imprimir a fatura no seu telemÃ³vel.\"\n",
    "Output: [Portugal]\n",
    "\n",
    "Input: \"Ð¡Ð¸Ð½Ð¾Ñ› ÑÐ°Ð¼ Ð²Ð¸Ð´Ð¸Ð¾ Ð´Ñ˜ÐµÐ²Ð¾Ñ˜ÐºÑƒ Ñƒ ÐŸÐ¾Ð´Ð³Ð¾Ñ€Ð¸Ñ†Ð¸.\"\n",
    "Output: [Montenegro]\n",
    "\n",
    "Input: \"é€™æ˜¯è‡ºç£æœ¬åœ°çš„é é¢ã€‚\"\n",
    "Output: [Taiwan]\n",
    "\n",
    "Now produce ONLY [Country] for the current sentence.\n",
    "\"\"\"},\n",
    "]\n",
    "\n",
    "lingua_locale = create_agent(model=llm, tools=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5da2a766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lingua_locale_answer(response: str) -> tuple[str, str]:\n",
    "    # TODO: Extract the country and \"none\" from the response\n",
    "    # only the first field is used, the second is a dummy field to make the return type consistent\n",
    "    # return country, \"none\"\n",
    "    import re\n",
    "    \n",
    "    # Look for pattern [country] in brackets\n",
    "    bracket_pattern = r'\\[([^\\]]+)\\]'\n",
    "    match = re.search(bracket_pattern, response)\n",
    "    \n",
    "    if match:\n",
    "        country = match.group(1).strip()\n",
    "        return country, \"none\"\n",
    "    \n",
    "    # Fallback: look for country name in response\n",
    "    lines = response.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        # Skip lines that are too long (likely explanations)\n",
    "        if len(line) > 100:\n",
    "            continue\n",
    "        # Clean the line\n",
    "        clean_line = line.strip().strip('[]\"\\'')\n",
    "        # Remove common prefixes\n",
    "        for prefix in ['The answer is', 'Answer:', 'Country:', 'The country is', 'This is from']:\n",
    "            clean_line = clean_line.replace(prefix, '').strip()\n",
    "        \n",
    "        # If we have a short, cleaned line, it's likely the country\n",
    "        if clean_line and len(clean_line) < 50:\n",
    "            # Remove any trailing punctuation\n",
    "            clean_line = clean_line.rstrip('.,;:')\n",
    "            return clean_line, \"none\"\n",
    "    \n",
    "    # Last resort: return empty string\n",
    "    return \"\", \"none\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e3291f",
   "metadata": {},
   "source": [
    "## Answering questions\n",
    "This part includes how we load the questions and generate the prediction in desired format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59f9861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geoguesser(q: dict, print_raw_response=False) -> tuple[str, str]:\n",
    "    if q[\"type\"] == \"GlobalTrekker\":\n",
    "        query = {\"role\": \"user\", \"content\": f\"Paragraph: {q['paragraph']}\"}\n",
    "        messages, agent, extractor = global_trekker_messages, global_trekker, extract_global_trekker_answer\n",
    "    elif q[\"type\"] == \"CulinaryDetective\":\n",
    "        query = {\"role\": \"user\", \"content\": f\"Ingredients: {q['ingredient']}. Description: {q['description']}\"}\n",
    "        messages, agent, extractor = culinary_detective_messages, culinary_detective, extract_culinary_detective_answer\n",
    "    else: #q[\"type\"] == \"LinguaLocale\":\n",
    "        query = {\"role\": \"user\", \"content\": f\"Sentence: {q['sentence']}\"}\n",
    "        messages, agent, extractor = lingua_locale_messages, lingua_locale, extract_lingua_locale_answer\n",
    "\n",
    "    response_all = agent.invoke({\"messages\": messages + [query]})\n",
    "    response = response_all[\"messages\"][-1].content\n",
    "    if print_raw_response: print(f\"{q['type']}: {response_all}\")\n",
    "    return extractor(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45950e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Here, we load the examples questions. Public/private set will be in the same format\n",
    "dataset_name = \"public.jsonl\"\n",
    "questions = []\n",
    "with open(dataset_name, \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        questions.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14091132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GlobalTrekker: {'messages': [SystemMessage(content='You are an expert in world geography and cultural geography. Given a descriptive paragraph, infer the most likely country and city with high precision.', additional_kwargs={}, response_metadata={}, id='16962dec-077c-4b5a-b261-c042a37adb4b'), HumanMessage(content='\\nRead the paragraph and extract:\\n- Country: use the full official country name (e.g., \"United States\", not \"USA\").\\n- City: a specific city if clearly indicated by clues (landmarks, neighborhoods, transit lines, local foods, dialects). If not identifiable, write \"Unknown\".\\n\\nStrict output format (no explanations):\\n[Country], [City]\\n\\nGuidelines:\\n- Prefer unique cues (local transit names, street names, postal formats, phone codes, currency, cuisine, sports clubs).\\n- If multiple cities fit, pick the single most likely one.\\n- If the paragraph is generic or only country-level, return \"Unknown\" for city.\\n- Do not include any extra text besides the bracketed pair.\\n\\nExamples:\\nInput: \"We walked along the Arno past the Ponte Vecchio before climbing to Piazzale Michelangelo.\"\\nOutput: [Italy], [Florence]\\n\\nInput: \"A red double-decker bus passed by the Thames near Westminster Abbey and Big Ben.\"\\nOutput: [United Kingdom], [London]\\n\\nInput: \"We grabbed Primanti\\'s near the confluence of the Allegheny and Monongahela.\"\\nOutput: [United States], [Pittsburgh]\\n\\nInput: \"I toured the Old Town and the Royal Mile before seeing the castle on the hill.\"\\nOutput: [United Kingdom], [Edinburgh]\\n\\nInput: \"We enjoyed maple syrup and poutine while skating on a canal in winter.\"\\nOutput: [Canada], [Ottawa]\\n', additional_kwargs={}, response_metadata={}, id='6a7567e6-adfb-472b-a4c3-f7ae8f273c9b'), HumanMessage(content='Paragraph: Uluru, also known as Ayers Rock, is a large sandstone monolith that is sacred to the Pitjantjatjara, the Aboriginal people of the area, known as the Aá¹‰angu. It is one of the most important indigenous sites of the country, being a popular destination for tourists since the 1930s. The area around the formation is home to many springs, waterholes, rock caves and ancient paintings. Uluru is listed as a UNESCO World Heritage Site.', additional_kwargs={}, response_metadata={}, id='04464dc4-6361-4363-8b2c-ad1badc9e054'), AIMessage(content='[Australia], [Unknown]', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 592, 'prompt_tokens': 441, 'total_tokens': 1033, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 576, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Cid4umRqOCiijijVhTrGiqEnWu2bf', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--93949415-0547-4d1e-af2a-ae7a9599be97-0', usage_metadata={'input_tokens': 441, 'output_tokens': 592, 'total_tokens': 1033, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 576}})]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Australia', 'Unknown')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test run on one question\n",
    "# You might want to save the raw response for debugging answer formatting/extraction\n",
    "# If the extracted answer seems off, check the raw response instead of running inference repeatedly\n",
    "geoguesser(questions[0], print_raw_response=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "339601c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170/170 [24:45<00:00,  8.74s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to public.txt (private set)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample script to generate answers\n",
    "from tqdm import tqdm\n",
    "answers = []\n",
    "for q in tqdm(questions):\n",
    "    try:\n",
    "        country, category = geoguesser(q)\n",
    "        answers.append(f\"{q['type']}\\t{country}\\t{category}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question {q}: {e}\")\n",
    "        answers.append(f\"{q['type']}\\tUnknown\\tUnknown\")\n",
    "\n",
    "with open(\"public.txt\", \"w\") as f:\n",
    "    for answer in answers:\n",
    "        f.write(answer + \"\\n\")\n",
    "\n",
    "print(\"Saved predictions to public.txt (private set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99774c89",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "This is how we calculate the scores on Gradescope (details subject to change, but the general logic will stay the same)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e478646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_match(answer, expectedAnswer):\n",
    "    score = 0.0\n",
    "    if expectedAnswer in answer:\n",
    "        score = len(expectedAnswer) / len(answer)\n",
    "    return score\n",
    "\n",
    "def exact_match(answer, expectedAnswer):\n",
    "    score = 0.0\n",
    "    if expectedAnswer == answer:\n",
    "        score = 1.0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f16b03ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GlobalTrekker Average Score: 0.8195\n",
      "CulinaryDetective Average Score: 0.5273\n",
      "LinguaLocale Average Score: 0.8778\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "for q in questions:\n",
    "    answers.append((q[\"type\"], q[\"country\"], q.get(\"city\", q.get(\"region\", \"\"))))\n",
    "with open(\"public.txt\", \"r\") as f:\n",
    "    preds = [line.split(\"\\t\") for line in f.readlines()]\n",
    "\n",
    "scores = {\"GlobalTrekker\": [], \"CulinaryDetective\": [], \"LinguaLocale\": []}\n",
    "for (q_type, exp_country, exp_place), (p_type, pred_country, pred_place) in zip(answers, preds):\n",
    "    assert q_type == p_type\n",
    "    country_score = soft_match(pred_country, exp_country)\n",
    "    category_score = 0.0\n",
    "    weights = [0.0, 0.0]\n",
    "    if q_type == \"GlobalTrekker\":\n",
    "        #  correct country -> 80%, correct country and city -> +20%\n",
    "        weights = [0.8, 0.2]\n",
    "        if country_score > 0:\n",
    "            if exp_place == \"None\": category_score = 1.0\n",
    "            else: category_score = soft_match(pred_place, exp_place)\n",
    "    elif q_type == \"CulinaryDetective\":\n",
    "        # correct country -> 60%, correct country and region -> +40%\n",
    "        weights = [0.6, 0.4]\n",
    "        if country_score > 0:\n",
    "            if exp_place == \"None\": category_score = 1.0\n",
    "            else: category_score = exact_match(pred_place, exp_place)\n",
    "    else: # LinguaLocale\n",
    "        # correct country -> 60%, matched official language -> +40%\n",
    "        weights = [0.6, 0.4]\n",
    "        if country_score > 0:\n",
    "            category_score = 1.0\n",
    "        else: # incorrect country. language match works only if pred_country is a clean answer\n",
    "            exp_langs = list_of_countries.get(exp_country, [])\n",
    "            pred_langs = list_of_countries.get(pred_country, [])\n",
    "            if any(lang in exp_langs for lang in pred_langs):\n",
    "                category_score = 1.0\n",
    "\n",
    "    score = weights[0] * country_score + weights[1] * category_score\n",
    "    scores[q_type].append(score)\n",
    "\n",
    "for q_type, score_list in scores.items():\n",
    "    avg_score = sum(score_list) / len(score_list)\n",
    "    print(f\"{q_type} Average Score: {avg_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Universal Environment",
   "language": "python",
   "name": "ml-universal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
