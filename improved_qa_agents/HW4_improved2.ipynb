{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f3fe1f",
   "metadata": {},
   "source": [
    "# HW4: QA Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c91ec1",
   "metadata": {},
   "source": [
    "## Dependencies and LLM Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42689839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain==1.0.5\n",
    "# !pip install langchain-core\n",
    "# !pip install langchain-community\n",
    "# !pip install faiss-cpu\n",
    "# !pip install kagglehub\n",
    "# !Install DuckDuckGo search dependency\n",
    "# !pip install -U ddgs\n",
    "device = \"cuda\"  # \"cpu\" or \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7af723c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the list of countries we are using (with their official languages)\n",
    "# Feel free to use it in your code\n",
    "list_of_countries = {}\n",
    "with open(\"countries_with_languages.tsv\", \"r\"  ) as f:\n",
    "    for line in f.readlines():\n",
    "        country, langs = line.strip().split(\"\\t\")\n",
    "        list_of_countries[country] = langs.split(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c06a7d",
   "metadata": {},
   "source": [
    "### Choice 1: OpenAI API\n",
    "\n",
    "The notebook's implementation is based on this.\n",
    "Feel free to change the model, and please keep track of your usage on the \"Usage\" page on [LiteLLM API webpage](https://ai-gateway.andrew.cmu.edu/ui/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b65e4808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5add67b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "import getpass, os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
    "openai_model_id = \"gpt-5\"\n",
    "openai_embmodel_id = \"azure/text-embedding-3-small\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=openai_model_id,\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=\"https://ai-gateway.andrew.cmu.edu/\"\n",
    ")\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=openai_embmodel_id,\n",
    "    api_key=os.environ['OPENAI_API_KEY'],\n",
    "    base_url='https://ai-gateway.andrew.cmu.edu/'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799b00ca",
   "metadata": {},
   "source": [
    "### Choice 2: Hugging Face Models\n",
    "\n",
    "You may also use Hugging Face models without API credits if you have available GPU resource. You might have to the change prompt templates according to your model choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac1bc014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-huggingface text-generation transformers google-search-results \n",
    "# !pip install numexpr langchainhub sentencepiece sentence-transformers jinja2 bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbf0fa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import getpass, os\n",
    "# from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline, HuggingFaceEmbeddings\n",
    "\n",
    "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass(\"Enter your Hugging Face API key: \")\n",
    "# hgf_model_id = \"Qwen/Qwen3-0.6B\"\n",
    "# hgf_embmodel_id = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# hgf_model = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=hgf_model_id,\n",
    "#     task=\"text-generation\",\n",
    "#     pipeline_kwargs=dict(\n",
    "#         max_new_tokens=128,\n",
    "#         do_sample=False,\n",
    "#     ),\n",
    "# )\n",
    "# hgf_llm = ChatHuggingFace(hgf_model)\n",
    "# hgf_embeddings = HuggingFaceEmbeddings(model_name=hgf_embmodel_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a37cbe",
   "metadata": {},
   "source": [
    "## Handling different type of questions\n",
    "\n",
    "Implement the answer formatting and extraction for each question type. You may change the prompt to fit your processing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e242dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6a4262",
   "metadata": {},
   "source": [
    "### ðŸ—ºï¸Global Trekker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc39e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Searches the web for information about locations, landmarks, and geographic features.\n",
    "    Useful for identifying cities and countries from descriptive clues.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from langchain_community.tools import DuckDuckGoSearchRun\n",
    "        search = DuckDuckGoSearchRun()\n",
    "        try:\n",
    "            results = search.run(query)\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            return f\"Search error: {str(e)}\"\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            \"DuckDuckGo search dependency missing or failed to load. \"\n",
    "            \"Install with: pip install -U ddgs. Error: \" + str(e)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bce1dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_trekker_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert in world geography and cultural geography. Given a descriptive paragraph, infer the most likely country and city with high precision.\"},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "Read the paragraph and extract:\n",
    "- Country: use the full official country name (e.g., \"United States\", not \"USA\").\n",
    "- City: a specific city if clearly indicated by clues (landmarks, neighborhoods, transit lines, local foods, dialects). If not identifiable, write \"Unknown\".\n",
    "\n",
    "Strict output format (no explanations):\n",
    "[Country], [City]\n",
    "\n",
    "Guidelines:\n",
    "- Prefer unique cues (local transit names, street names, postal formats, phone codes, currency, cuisine, sports clubs).\n",
    "- If multiple cities fit, pick the single most likely one.\n",
    "- If the paragraph is generic or only country-level, return \"Unknown\" for city.\n",
    "- Do not include any extra text besides the bracketed pair.\n",
    "\n",
    "Examples:\n",
    "Input: \"We walked along the Arno past the Ponte Vecchio before climbing to Piazzale Michelangelo.\"\n",
    "Output: [Italy], [Florence]\n",
    "\n",
    "Input: \"A red double-decker bus passed by the Thames near Westminster Abbey and Big Ben.\"\n",
    "Output: [United Kingdom], [London]\n",
    "\n",
    "Input: \"We grabbed Primanti's near the confluence of the Allegheny and Monongahela.\"\n",
    "Output: [United States], [Pittsburgh]\n",
    "\n",
    "Input: \"I toured the Old Town and the Royal Mile before seeing the castle on the hill.\"\n",
    "Output: [United Kingdom], [Edinburgh]\n",
    "\n",
    "Input: \"We enjoyed maple syrup and poutine while skating on a canal in winter.\"\n",
    "Output: [Canada], [Ottawa]\n",
    "\"\"\"},\n",
    "]\n",
    "global_trekker = create_agent(model=llm, tools=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85342fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_global_trekker_answer(response: str) -> tuple[str, str]:\n",
    "    # TODO: Extract the country and city from the response\n",
    "    # return country, city\n",
    "    import re\n",
    "    \n",
    "    chars = r'\\[([^\\]]+)\\]\\s*,\\s*\\[([^\\]]+)\\]'\n",
    "    match = re.search(chars, response)\n",
    "    \n",
    "    if match:\n",
    "        country = match.group(1).strip()\n",
    "        city = match.group(2).strip()\n",
    "        return country, city\n",
    "    \n",
    "    lines = response.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        if len(line) > 100:\n",
    "            continue\n",
    "        if ',' in line:\n",
    "            parts = line.split(',')\n",
    "            if len(parts) >= 2:\n",
    "                country = parts[0].strip().strip('[]\"\\'')\n",
    "                city = parts[1].strip().strip('[]\"\\'')\n",
    "                for prefix in ['The answer is', 'Answer:', 'Location:', 'Country:', 'City:']:\n",
    "                    country = country.replace(prefix, '').strip()\n",
    "                    city = city.replace(prefix, '').strip()\n",
    "                return country, city\n",
    "    \n",
    "    return \"\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd5210d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('AAA', 'BBB')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test run your extration function before using it in the main loop!\n",
    "extract_global_trekker_answer(\"AAA, BBB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78ab9c1",
   "metadata": {},
   "source": [
    "### ðŸ½ï¸Culinary Detective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a359380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "\n",
    "def gather_recipe_data(kaggledataset: str) -> list[str]:\n",
    "    dataset_path = kagglehub.dataset_download(kaggledataset)\n",
    "    df = pd.read_csv(f\"{dataset_path}/Receipes from around the world.csv\", encoding='latin-1')\n",
    "    \n",
    "    recipes = []\n",
    "    for _, row in df.iterrows():\n",
    "        recipe_parts = []\n",
    "        for col in df.columns:\n",
    "            value = row[col]\n",
    "            if pd.notna(value) and str(value).strip():\n",
    "                recipe_parts.append(f\"{col}: {value}\")\n",
    "        \n",
    "        recipe_text = \". \".join(recipe_parts)\n",
    "        recipes.append(recipe_text)\n",
    "    \n",
    "    return recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b33c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain.tools import tool\n",
    "\n",
    "recipes = gather_recipe_data(\"prajwaldongre/collection-of-recipes-around-the-world\")\n",
    "docs = [Document(page_content=recipe) for recipe in recipes]\n",
    "vector = FAISS.from_documents(docs, embeddings)\n",
    "retriever = vector.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d53c0b0",
   "metadata": {},
   "source": [
    "## RAG Tool\n",
    "I created the following:\n",
    "- a folder to store embeddings and faiss index\n",
    "- a rag pipeline file\n",
    "- a file that exposes the rag pipeline as a tool. \n",
    "- I am importing the tool in here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cafce3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/codespace/nlp_assignments/improved_qa_agents/rag_system/rag_pipeline.py:13: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embeddings = HuggingFaceEmbeddings(\n",
      "2025-12-06 10:15:18.336970: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765016118.365881   55052 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765016118.377381   55052 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-12-06 10:15:18.587144: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "from rag_system.rag_pipeline import CulinaryRAG\n",
    "\n",
    "rag = CulinaryRAG()\n",
    "retriever = rag.load_index() \n",
    "\n",
    "@tool\n",
    "def retrieve_culinary_context(query: str):\n",
    "    \"\"\"\n",
    "    Retrieves culinary information relevant to country/region origin detection.\n",
    "    Takes a descriptive query (ingredients, cooking method, spices) and performs vector retrieval.\n",
    "    \"\"\"\n",
    "    docs = retriever.invoke(query)\n",
    "    return \"\\n\".join([d.page_content for d in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d3307f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/sagemaker-user/.cache/kagglehub/datasets/prajwaldongre/collection-of-recipes-around-the-world/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"prajwaldongre/collection-of-recipes-around-the-world\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44bd8a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def retrieve_recipes(query: str):\n",
    "  \"\"\"\n",
    "  Retrieves recipes based on a search query.\n",
    "  \"\"\"\n",
    "  return retriever.invoke(query)\n",
    "\n",
    "culinary_detective_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"\"\"\n",
    "You are an expert culinary anthropologist. From ingredients and a short description, identify the country and the SPECIFIC region within that country where the dish is most associated.\n",
    "\n",
    "You can consult a retrieval tool bound to this agent (retrieve_culinary_context). Use it when helpful; otherwise reason from your knowledge of ingredients, techniques, and named dishes.\n",
    "\"\"\"},\n",
    "    {\"role\": \"human\", \"content\": \"\"\"\n",
    "Task:\n",
    "Return ONLY a single line in the exact format: [Country], [Region]\n",
    "- Country: full official name (e.g., \"United States\", not \"USA\").\n",
    "- Region: a specific intra-country region (e.g., North, South, East, West, Central, Northeast, etc.).\n",
    "- Use \"All\" only if the dish is truly nationwide.\n",
    "- If no region is identifiable, write \"Unknown\".\n",
    "- Do NOT include any explanation before or after the bracketed answer.\n",
    "\n",
    "Cues to consider:\n",
    "- Ingredients (grains, staple flours, spice blends), cooking methods, named dishes, iconic sides.\n",
    "- Example mappings:\n",
    "  * Brazil: pÃ£o de queijo / tapioca flour / queijo minas â†’ [Brazil], [South]\n",
    "  * Japan: sushi / rice vinegar / tempura â†’ [Japan], [All]\n",
    "  * India: dosa / idli / coconut â†’ [India], [South]; naan / paneer / tandoor â†’ [India], [North]\n",
    "  * Ethiopia: teff / injera / wat / berbere â†’ [Ethiopia], [Unknown]\n",
    "  * China: dim sum â†’ [China], [South]; hot pot (Sichuan/Chongqing-style) â†’ [China], [West]\n",
    "  * Thailand: khao soi â†’ [Thailand], [North]; som tam â†’ [Thailand], [Northeast]\n",
    "\n",
    "Few-shot examples:\n",
    "Input: \"Fermented teff flatbread served with spicy stews (wat) and berbere.\"\n",
    "Output: [Ethiopia], [Unknown]\n",
    "\n",
    "Input: \"Cheese bread made with tapioca starch, typical with churrasco in the south.\"\n",
    "Output: [Brazil], [South]\n",
    "\n",
    "Input: \"Batter of rice and urad dal, steamed into soft cakes, served with coconut chutney.\"\n",
    "Output: [India], [South]\n",
    "\n",
    "Input: \"Assorted small bites with tea in bamboo steamers, hallmark of Cantonese cuisine.\"\n",
    "Output: [China], [South]\n",
    "\n",
    "Now produce ONLY the answer for the current input as [Country], [Region].\n",
    "\"\"\"},\n",
    "]\n",
    "\n",
    "culinary_detective = create_agent(model=llm, tools=[retrieve_culinary_context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "921d5b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [SystemMessage(content='\\nYou are an expert culinary anthropologist. From ingredients and a short description, identify the country and the SPECIFIC region within that country where the dish is most associated.\\n\\nYou can consult a retrieval tool bound to this agent (retrieve_culinary_context). Use it when helpful; otherwise reason from your knowledge of ingredients, techniques, and named dishes.\\n', additional_kwargs={}, response_metadata={}, id='26472349-a4d7-4947-8ad4-b04953455e00'), HumanMessage(content='\\nTask:\\nReturn ONLY a single line in the exact format: [Country], [Region]\\n- Country: full official name (e.g., \"United States\", not \"USA\").\\n- Region: a specific intra-country region (e.g., North, South, East, West, Central, Northeast, etc.).\\n- Use \"All\" only if the dish is truly nationwide.\\n- If no region is identifiable, write \"Unknown\".\\n- Do NOT include any explanation before or after the bracketed answer.\\n\\nCues to consider:\\n- Ingredients (grains, staple flours, spice blends), cooking methods, named dishes, iconic sides.\\n- Example mappings:\\n  * Brazil: pÃ£o de queijo / tapioca flour / queijo minas â†’ [Brazil], [South]\\n  * Japan: sushi / rice vinegar / tempura â†’ [Japan], [All]\\n  * India: dosa / idli / coconut â†’ [India], [South]; naan / paneer / tandoor â†’ [India], [North]\\n  * Ethiopia: teff / injera / wat / berbere â†’ [Ethiopia], [Unknown]\\n  * China: dim sum â†’ [China], [South]; hot pot (Sichuan/Chongqing-style) â†’ [China], [West]\\n  * Thailand: khao soi â†’ [Thailand], [North]; som tam â†’ [Thailand], [Northeast]\\n\\nFew-shot examples:\\nInput: \"Fermented teff flatbread served with spicy stews (wat) and berbere.\"\\nOutput: [Ethiopia], [Unknown]\\n\\nInput: \"Cheese bread made with tapioca starch, typical with churrasco in the south.\"\\nOutput: [Brazil], [South]\\n\\nInput: \"Batter of rice and urad dal, steamed into soft cakes, served with coconut chutney.\"\\nOutput: [India], [South]\\n\\nInput: \"Assorted small bites with tea in bamboo steamers, hallmark of Cantonese cuisine.\"\\nOutput: [China], [South]\\n\\nNow produce ONLY the answer for the current input as [Country], [Region].\\n', additional_kwargs={}, response_metadata={}, id='8a020ab5-25e5-4fbc-bd1c-648feba29800'), HumanMessage(content='The dish is spicy, coconut-based, and served with rice.', additional_kwargs={}, response_metadata={}, id='9c8556f1-5aa5-472c-9945-a72c8adea499'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 932, 'prompt_tokens': 659, 'total_tokens': 1591, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 896, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CjjcGj8s2RFTwLPtI4gm7A8KsDwyx', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--dae36746-907b-4db5-a523-070d0b71d5d6-0', tool_calls=[{'name': 'retrieve_culinary_context', 'args': {'query': 'Spicy coconut-based dish served with rice region association'}, 'id': 'call_9fox8Wd0q1ASJ1cITK1ASi3S', 'type': 'tool_call'}], usage_metadata={'input_tokens': 659, 'output_tokens': 932, 'total_tokens': 1591, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 896}}), ToolMessage(content='RecipeId: 123776\\nName: South Indian Coconut Rice\\nAuthorId: 83400\\nAuthorName: Daydream\\nCookTime: PT30M\\nPrepTime: PT20M\\nTotalTime: PT50M\\nDatePublished: 2005-05-26T22:22:00Z\\nDescription: Lovely with curries, particularly sea-food, this rice dish from Tamil Nadu is seasoned with lightly roasted coconut, cilantro, curry leaves, dried red chillies, and mustard seeds. It also contains a small amount of chana dal and urad dal, which, when roasted, provide a nutty taste and texture. If you cannot obtain fresh coconut, soak 1 1/4 cups of unsweetened, desiccated coconut in warm water (to barely cover) for 1 hour then drain and squeeze dry.\\nImages: character(0)\\nRecipeCategory: Curries\\nKeywords: c(\"Rice\", \"Coconut\", \"Fruit\", \"Nuts\", \"Asian\", \"Indian\", \"< 60 Mins\", \"Stove Top\")\\nRecipeIngredientQuantities: c(\"2\", \"3\", \"6\", \"1 1/2\", \"1 1/2\", \"1\", \"15\", \"1\", \"1/4\", \"1 3/4\", \"2\")\\nRecipeIngredientParts: c(\"basmati rice\", \"coconut oil\", \"channa dal\", \"Urad Dal\", \"brown mustard seeds\", \"curry leaves\", \"salt\", \"asafoetida powder\", \"coconut\", \"fresh cilantro\")\\nAggregatedRating: 4.5\\nReviewCount: 3\\nCalories: 716.8\\nFatContent: 37.5\\nSaturatedFatContent: 30.7\\nCholesterolContent: 0\\nSodiumContent: 608.5\\nCarbohydrateContent: 88.2\\nFiberContent: 11.1\\nSugarContent: 7.3\\nProteinContent: 12\\nRecipeServings: 4\\nRecipeYield: NA\\nRecipeInstructions: c(\"Wash the rice well and cook until tender in a rice cooker or by the absorption method.\", \"While the rice is cooking, heat 3 tablespoons oil in a medium-sized pan or skillet, and when hot add the chillies, chana dal, urad dal and mustard seeds. Stir and fry until the chillies darken and the dals turn golden brown.\", \"Add the curry leaves, salt, asafetida and coconut, and stir and fry for a minute or two before reducing heat.\", \"Gently cook the coconut mixture until the coconut turns a toasted reddish-brown and becomes quite crisp. It burns easily, so don\\'t take your eyes off it for a second, and keep the coconut moving by stirring and tossing constantly.\", \\n\"As soon as the coconut is the right colour and texture, turn off the heat. If the rice is not ready, transfer the coconut mixture to a bowl, so that it doesn\\'t continue browning in the hot pan.\", \"Mix the coconut mixture gently through the hot, cooked rice, then stir through the chopped cilantro and serve immediately.\")\\nRecipeId: 27994\\nName: Thai Red Curry\\nAuthorId: 6357\\nAuthorName: Charishma_Ramchanda\\nCookTime: PT15M\\nPrepTime: PT15M\\nTotalTime: PT30M\\nDatePublished: 2002-05-09T18:36:00Z\\nDescription: This is a spicy combination of vegetables with a coconut flavour! It makes for a hearty lunch/dinner and is one of those dishes that imparts its own elegance to the table.\\nImages: character(0)\\nRecipeCategory: Lunch/Snacks\\nKeywords: c(\"Soy/Tofu\", \"Beans\", \"Thai\", \"Asian\", \"Microwave\", \"< 30 Mins\")\\nRecipeIngredientQuantities: c(\"3/4\", \"3/4\", \"3/4\", \"1/2\", \"1/2\", \"1/2\", \"1/2\", \"1/4\", \"1/2\", \"1/2\", \"2\", \"1/2\", \"1/2\", \"2\", \"1\", NA, \"4\", \"1\", \"1\", \"1/4\", \"2\", \"2\", \"1/4\", \"1/4\", \"1/4\", \"1\", \"1\")\\nRecipeIngredientParts: c(\"broccoli floret\", \"cauliflower floret\", \"green pepper\", \"tofu\", \"mushroom\", \"spinach\", \"coconut cream\", \"spring onions\", \"soy sauce\", \"sugar\", \"cornflour\", \"salt\", \"red chilies\", \"ginger\", \"garlic clove\", \"onion\", \"lemongrass\", \"cilantro\", \"white pepper powder\", \"salt\", \"cumin seed\", \"spring onion\", \"spring onion leaves\")\\nAggregatedRating: NA\\nReviewCount: NA\\nCalories: 594.5\\nFatContent: 30\\nSaturatedFatContent: 13.8\\nCholesterolContent: 0\\nSodiumContent: 448.1\\nCarbohydrateContent: 77.6\\nFiberContent: 6.7\\nSugarContent: 50.9\\nProteinContent: 12.7\\nRecipeServings: 2\\nRecipeYield: NA\\nRecipeInstructions: c(\"Soak red chillies in water for 15 minutes.\", \"Coarse grind with other ingredients.\", \"In a glass dish, microwave oil on HIGH for 2 minutes uncovered.\", \"Add spring onion, bell peppers and curry paste.\", \"Mix and microwave on HIGH for 2 minutes uncovered.\", \"Add Broccoli, babycorn, cauliflower and 2 tbsp water.\", \"Mix and microwave on HIGH for 4 minutes uncovered.\", \"Allow standing time of 2 minutes.\", \"Add spinach, mushrooms and tofu.\", \"Mix and microwave on HIGH for 3 minutes covered.\", \"Allow standing time of 2 minutes.\", \\n\"Add coconut cream, cornflour mixed with 1 cup water, salt, sugar and soya sauce.\", \"Mix and microwave on MEDIUM 50% for 4 minutes uncovered or until curry thickens to the required consistency.\", \"Serve garnished with spring onion and spring onion leaves.\", \"Serve hot with Thai green rice or steamed rice.\", \"Guideline- For best thickening results, always microwave gravy dishes on MEDIUM power.\")\\nRecipeId: 329270\\nName: Urap\\nAuthorId: 746803\\nAuthorName: Coasty\\nCookTime: PT15M\\nPrepTime: PT15M\\nTotalTime: PT30M\\nDatePublished: 2008-10-06T18:48:00Z\\nDescription: This is an Indonesian vegetable dish which can be served at room temperature or hot.  Most of the ingredients are easy to find.  The trasi or belachan(shrimp paste) can usually be found at the asian grocers.  The fresh coconut can be substituted for desicated coconut, but you will need to sprinkle 1/4 boiling water over the coconut to plump it up. The chilli paste can be left out.\\nImages: character(0)\\nRecipeCategory: Corn\\nKeywords: c(\"Vegetable\", \"Indonesian\", \"Asian\", \"Spicy\", \"< 30 Mins\", \"Inexpensive\")\\nRecipeIngredientQuantities: c(\"200\", \"150\", \"100\", \"1\", \"1\", \"1\", \"1\", \"2\", \"1\", NA)\\nRecipeIngredientParts: c(\"cabbage\", \"carrots\", \"bean sprouts\", \"corn kernel\", \"lime juice\", \"shrimp paste\", \"salt\")\\nAggregatedRating: NA\\nReviewCount: NA\\nCalories: 98.1\\nFatContent: 4.8\\nSaturatedFatContent: 4\\nCholesterolContent: 0\\nSodiumContent: 420.2\\nCarbohydrateContent: 13.9\\nFiberContent: 3.8\\nSugarContent: 4.2\\nProteinContent: 2.6\\nRecipeServings: 6\\nRecipeYield: NA\\nRecipeInstructions: c(\"Shred the cabbage.  Peel carrots and cut into 2cm battons. Wash beanshoots.\", \"Form trasi (shrimp paste)  into a ball and pierce with a skewer.  Hold over a flame (either stove burner or candle) and scorce slightly.  Cool slightly.\", \"Combine coconut, lime juice, chilli paste salt and trasi. Using the back of a large spoon mash together, especially the trasi until well combined.\", \"Traditional  this is then folded in a banana leaf package and steam.  Although you can place in a ramekin or wrap in baking paper.\", \\n\"Steam the vegetables each separately until just done, placing the coconut mix with the carrots.\", \"Combine all the vegetables a bowl and stir through the coconut mix well.\", \"This dish can be served either hot or at room temperature.\", \"Be careful - if using fresh coconut this dish should be consumed within a couple of hours as the fresh coconut will turn sour and be inedible.\")\\nRecipeId: 149821\\nName: Stir-Fried Coconut Noodles\\nAuthorId: 278369\\nAuthorName: Little Mimi\\nCookTime: NA\\nPrepTime: PT45M\\nTotalTime: PT45M\\nDatePublished: 2006-01-01T20:50:00Z\\nDescription: Here is a delicious, easy one-dish meal.  The flavors in the dish are amazing--you\\'ll be transported to Southeast Asia.  The less common ingredients should be available in the Asian section of the supermarket.  If you like some heat, add some red or green curry paste.  This dish tastes best the first day.\\nImages: character(0)\\nRecipeCategory: Coconut\\nKeywords: c(\"Fruit\", \"Nuts\", \"Asian\", \"Savory\", \"< 60 Mins\", \"Stir Fry\", \"Inexpensive\")\\nRecipeIngredientQuantities: c(\"1\", \"3\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", NA, NA, \"1/4\")\\nRecipeIngredientParts: c(\"linguine\", \"canola oil\", \"ground pork\", \"ground turkey\", \"red bell pepper\", \"eggplant\", \"garlic\", \"unsweetened coconut milk\", \"green curry paste\", \"red curry paste\", \"soy sauce\", \"salt\", \"fresh ground black pepper\", \"fresh cilantro\")\\nAggregatedRating: 4\\nReviewCount: 1\\nCalories: 1070.3\\nFatContent: 56.9\\nSaturatedFatContent: 29.2\\nCholesterolContent: 106.7\\nSodiumContent: 305.3\\nCarbohydrateContent: 103.1\\nFiberContent: 4.4\\nSugarContent: 2.6\\nProteinContent: 36.2\\nRecipeServings: 4\\nRecipeYield: NA\\nRecipeInstructions: c(\"If using rice noodles, place them in a bowl and cover them with very hot water.  Soak them until read to add to the stir fry in Step 4.  If using linguine, bring a large pot of water to a boil, salt it, and cook paste until it is nearly but not quite done.  Drain, and rinse in cold water.\", \"Meanwhile, place 1 tablespoon of the oil in a large skillet or wok; turn heat to high.  A minute later, add meat, and cook, stirring occasionally, until browned, about 5 minutes.  Remove with slotted spoon; set aside.\", \\n\"Add another tablespoon of oil to the skilled, followed by the bell pepper and eggplant.  Cook over medium-high heat, stirring occasionally, until both are browned and tender, about 10 minutes.  Remove with slotted spoon, and combine with meat.\", \"Add remaining tablespoon of oil, followed immediately by the garlic and about 30 seconds later, the coconut milk.  If desired, add curry paste at ths point.  Cook over medium-high heat, stirring and scraping with a wooden spoon, for about 1 minute.  Add drained noodles along with meat and vegetables, and cook until noodles absorb most of the coconut milk, about 3 minutes.\", \\n\"Season with nam pla to taste, then add plenty of black pepper.  Garnish with cilantro, and serve.\")', name='retrieve_culinary_context', id='d5a0ab79-ccc4-42eb-a2ee-eea52bbaf500', tool_call_id='call_9fox8Wd0q1ASJ1cITK1ASi3S'), AIMessage(content='[India], [South]', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1680, 'prompt_tokens': 3303, 'total_tokens': 4983, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1664, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CjjcRUbS3YI1XtCkBzQo8xuJCSthf', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--64ce9843-2bfa-4b95-8745-1d4843174f98-0', usage_metadata={'input_tokens': 3303, 'output_tokens': 1680, 'total_tokens': 4983, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1664}})]}\n"
     ]
    }
   ],
   "source": [
    "result = culinary_detective.invoke({\n",
    "    \"messages\": culinary_detective_messages + [\n",
    "        {\"role\": \"human\", \"content\": \"The dish is spicy, coconut-based, and served with rice.\"}\n",
    "    ]\n",
    "})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e6a54a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_culinary_detective_answer(response: str) -> tuple[str, str]:\n",
    "    # TODO: Extract the country and region from the response\n",
    "    # return country, region\n",
    "    import re\n",
    "    \n",
    "    chars = r'\\[([^\\]]+)\\]\\s*,\\s*\\[([^\\]]+)\\]'\n",
    "    match = re.search(chars, response)\n",
    "    \n",
    "    if match:\n",
    "        country = match.group(1).strip()\n",
    "        region = match.group(2).strip()\n",
    "        return country, region\n",
    "    \n",
    "    lines_split = response.strip().split('\\n')\n",
    "    for line in lines_split:\n",
    "        if len(line) > 100:\n",
    "            continue\n",
    "        if ',' in line:\n",
    "            parts = line.split(',')\n",
    "            if len(parts) >= 2:\n",
    "                country = parts[0].strip().strip('[]\"\\'')\n",
    "                region = parts[1].strip().strip('[]\"\\'')\n",
    "                for prefix in ['The answer is', 'Answer:', 'Country:', 'Region:']:\n",
    "                    country = country.replace(prefix, '').strip()\n",
    "                    region = region.replace(prefix, '').strip()\n",
    "                return country, region\n",
    "    \n",
    "    return \"\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9400a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_culinary_detective_answer(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86672811",
   "metadata": {},
   "source": [
    "### ðŸ‘„Lingua Locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ad20b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "lingua_locale_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"\"\"\n",
    "You are an expert in languages, scripts, orthography, and regional vocabulary. Determine which country's website or text a sentence most likely comes from.\n",
    "\"\"\"},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "You must ONLY return a single line: [Country]. You should use the full official country name (e.g., \"United States\", not \"USA\"). \n",
    "- You should not add any other explanations. Only output the name of the country.\n",
    "\"\"\"},\n",
    "]\n",
    "\n",
    "lingua_locale = create_agent(model=llm, tools=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5da2a766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lingua_locale_answer(response: str) -> tuple[str, str]:\n",
    "    # TODO: Extract the country and \"none\" from the response\n",
    "    # only the first field is used, the second is a dummy field to make the return type consistent\n",
    "    # return country, \"none\"\n",
    "    import re\n",
    "    \n",
    "    chars = r'\\[([^\\]]+)\\]'\n",
    "    match = re.search(chars, response)\n",
    "    \n",
    "    if match:\n",
    "        country = match.group(1).strip()\n",
    "        return country, \"none\"\n",
    "    \n",
    "    lines_split = response.strip().split('\\n')\n",
    "    for line in lines_split:\n",
    "        if len(line) > 100:\n",
    "            continue\n",
    "        clean_line = line.strip().strip('[]\"\\'')\n",
    "        for prefix in ['The answer is', 'Answer:', 'Country:', 'The country is', 'This is from']:\n",
    "            clean_line = clean_line.replace(prefix, '').strip()\n",
    "        \n",
    "        if clean_line and len(clean_line) < 50:\n",
    "            clean_line = clean_line.rstrip('.,;:')\n",
    "            return clean_line, \"none\"\n",
    "    \n",
    "    return \"\", \"none\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e3291f",
   "metadata": {},
   "source": [
    "## Answering questions\n",
    "This part includes how we load the questions and generate the prediction in desired format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59f9861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geoguesser(q: dict, print_raw_response=False) -> tuple[str, str]:\n",
    "    if q[\"type\"] == \"GlobalTrekker\":\n",
    "        query = {\"role\": \"user\", \"content\": f\"Paragraph: {q['paragraph']}\"}\n",
    "        messages, agent, extractor = global_trekker_messages, global_trekker, extract_global_trekker_answer\n",
    "    elif q[\"type\"] == \"CulinaryDetective\":\n",
    "        query = {\"role\": \"user\", \"content\": f\"Ingredients: {q['ingredient']}. Description: {q['description']}\"}\n",
    "        messages, agent, extractor = culinary_detective_messages, culinary_detective, extract_culinary_detective_answer\n",
    "    else: #q[\"type\"] == \"LinguaLocale\":\n",
    "        query = {\"role\": \"user\", \"content\": f\"Sentence: {q['sentence']}\"}\n",
    "        messages, agent, extractor = lingua_locale_messages, lingua_locale, extract_lingua_locale_answer\n",
    "\n",
    "    response_all = agent.invoke({\"messages\": messages + [query]})\n",
    "    response = response_all[\"messages\"][-1].content\n",
    "    if print_raw_response: print(f\"{q['type']}: {response_all}\")\n",
    "    return extractor(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45950e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Here, we load the examples questions. Public/private set will be in the same format\n",
    "dataset_name = \"private.jsonl\"\n",
    "questions = []\n",
    "with open(dataset_name, \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        questions.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "14091132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinguaLocale: {'messages': [SystemMessage(content=\"\\nYou are an expert in languages, scripts, orthography, and regional vocabulary. Determine which country's website or text a sentence most likely comes from.\\n\", additional_kwargs={}, response_metadata={}, id='eb91a6d1-a4ee-476e-a1cc-a170f4a5dda2'), HumanMessage(content='\\nYou must ONLY return a single line: [Country]. You should use the full official country name (e.g., \"United States\", not \"USA\"). \\n- You should not add any other explanations. Only output the name of the country.\\n', additional_kwargs={}, response_metadata={}, id='56e3d8b6-a207-462f-b894-620142af5380'), HumanMessage(content='Sentence: No zwee Joer Iwwerleeungen, Diskussiounen, Debaten, Dialogen, Diagnosen a Consultatiounen ass de Plang konkretisÃ©iert ginn.', additional_kwargs={}, response_metadata={}, id='b7211937-c28a-4e81-9881-cc2b254daddb'), AIMessage(content='Luxembourg', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 205, 'prompt_tokens': 130, 'total_tokens': 335, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 192, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CjkfKctVTUGQTCFZ1KdTPu7Sxwckz', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--a62d5e6b-c2eb-4471-8018-7846c73b3a6e-0', usage_metadata={'input_tokens': 130, 'output_tokens': 205, 'total_tokens': 335, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 192}})]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Luxembourg', 'none')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test run on one question\n",
    "# You might want to save the raw response for debugging answer formatting/extraction\n",
    "# If the extracted answer seems off, check the raw response instead of running inference repeatedly\n",
    "geoguesser(questions[0], print_raw_response=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "339601c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/174 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 174/174 [23:13<00:00,  8.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to public.txt (private set)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample script to generate answers\n",
    "from tqdm import tqdm\n",
    "answers = []\n",
    "for q in tqdm(questions):\n",
    "    try:\n",
    "        country, category = geoguesser(q)\n",
    "        answers.append(f\"{q['type']}\\t{country}\\t{category}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question {q}: {e}\")\n",
    "        answers.append(f\"{q['type']}\\tUnknown\\tUnknown\")\n",
    "\n",
    "with open(\"private.txt\", \"w\") as f:\n",
    "    for answer in answers:\n",
    "        f.write(answer + \"\\n\")\n",
    "\n",
    "print(\"Saved predictions to public.txt (private set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99774c89",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "This is how we calculate the scores on Gradescope (details subject to change, but the general logic will stay the same)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e478646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_match(answer, expectedAnswer):\n",
    "    score = 0.0\n",
    "    if expectedAnswer in answer:\n",
    "        score = len(expectedAnswer) / len(answer)\n",
    "    return score\n",
    "\n",
    "def exact_match(answer, expectedAnswer):\n",
    "    score = 0.0\n",
    "    if expectedAnswer == answer:\n",
    "        score = 1.0\n",
    "    return score    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f16b03ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GlobalTrekker Average Score: 0.8470\n",
      "CulinaryDetective Average Score: 0.5345\n",
      "LinguaLocale Average Score: 0.9329\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "for q in questions:\n",
    "    answers.append((q[\"type\"], q[\"country\"], q.get(\"city\", q.get(\"region\", \"\"))))\n",
    "with open(\"public.txt\", \"r\") as f:\n",
    "    preds = [line.split(\"\\t\") for line in f.readlines()]\n",
    "\n",
    "scores = {\"GlobalTrekker\": [], \"CulinaryDetective\": [], \"LinguaLocale\": []}\n",
    "for (q_type, exp_country, exp_place), (p_type, pred_country, pred_place) in zip(answers, preds):\n",
    "    assert q_type == p_type\n",
    "    country_score = soft_match(pred_country, exp_country)\n",
    "    category_score = 0.0\n",
    "    weights = [0.0, 0.0]\n",
    "    if q_type == \"GlobalTrekker\":\n",
    "        #  correct country -> 80%, correct country and city -> +20%\n",
    "        weights = [0.8, 0.2]\n",
    "        if country_score > 0:\n",
    "            if exp_place == \"None\": category_score = 1.0\n",
    "            else: category_score = soft_match(pred_place, exp_place)\n",
    "    elif q_type == \"CulinaryDetective\":\n",
    "        # correct country -> 60%, correct country and region -> +40%\n",
    "        weights = [0.6, 0.4]\n",
    "        if country_score > 0:\n",
    "            if exp_place == \"None\": category_score = 1.0\n",
    "            else: category_score = exact_match(pred_place, exp_place)\n",
    "    else: # LinguaLocale\n",
    "        # correct country -> 60%, matched official language -> +40%\n",
    "        weights = [0.6, 0.4]\n",
    "        if country_score > 0:\n",
    "            category_score = 1.0\n",
    "        else: # incorrect country. language match works only if pred_country is a clean answer\n",
    "            exp_langs = list_of_countries.get(exp_country, [])\n",
    "            pred_langs = list_of_countries.get(pred_country, [])\n",
    "            if any(lang in exp_langs for lang in pred_langs):\n",
    "                category_score = 1.0\n",
    "\n",
    "    score = weights[0] * country_score + weights[1] * category_score\n",
    "    scores[q_type].append(score)\n",
    "\n",
    "for q_type, score_list in scores.items():\n",
    "    avg_score = sum(score_list) / len(score_list)\n",
    "    print(f\"{q_type} Average Score: {avg_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "603e38d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(num_exact: int, num_total: int) -> float:\n",
    "    acc_score = 0.0\n",
    "    if num_total > 0:\n",
    "        score = (num_exact / num_total)*100\n",
    "    return score\n",
    "\n",
    "# Let's calculate Precision, Recall, F1\n",
    "def calculate_metrics(tp, fp, fn):\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "65d74ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GlobalTrekker Accuracy (% exact): 0.0000 | exact=0 / total=64\n",
      "CulinaryDetective Accuracy (% exact): 0.0000 | exact=0 / total=56\n",
      "LinguaLocale Accuracy (% exact): 0.0000 | exact=0 / total=54\n"
     ]
    }
   ],
   "source": [
    "# Per-type exact-match accuracy (robust)\n",
    "from collections import Counter\n",
    "\n",
    "# 1) Build expected answers per question with safe defaults\n",
    "exp = []\n",
    "for q in questions:\n",
    "    q_type = q.get(\"type\", \"Unknown\")\n",
    "    exp_country = q.get(\"country\", \"Unknown\")\n",
    "    if q_type == \"GlobalTrekker\":\n",
    "        exp_place = q.get(\"city\", \"Unknown\")\n",
    "    elif q_type == \"CulinaryDetective\":\n",
    "        exp_place = q.get(\"region\", \"Unknown\")\n",
    "    else:\n",
    "        exp_place = \"\"\n",
    "    exp.append((q_type, exp_country, exp_place))\n",
    "\n",
    "# 2) Read predictions and pad/truncate to 3 fields\n",
    "preds_raw = []\n",
    "with open(\"public.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        parts = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        parts = (parts + [\"Unknown\", \"Unknown\"])[:3]\n",
    "        preds_raw.append(parts)\n",
    "\n",
    "# 3) Count totals per type\n",
    "type_counts = Counter([t for t,_,_ in exp])\n",
    "\n",
    "# 4) Exact-match counters per type\n",
    "exact_counts = {\"GlobalTrekker\": 0, \"CulinaryDetective\": 0, \"LinguaLocale\": 0}\n",
    "\n",
    "# 5) Iterate and count exact matches according to task rules\n",
    "for (q_type, exp_country, exp_place), (p_type, pred_country, pred_place) in zip(exp, preds_raw):\n",
    "    if q_type != p_type:\n",
    "        # Skip mismatches to avoid misaligned files\n",
    "        continue\n",
    "    if q_type == \"GlobalTrekker\":\n",
    "        if exact_match(pred_country, exp_country) == 1.0 and exact_match(pred_place, exp_place) == 1.0:\n",
    "            exact_counts[q_type] += 1\n",
    "    elif q_type == \"CulinaryDetective\":\n",
    "        if exact_match(pred_country, exp_country) == 1.0 and exact_match(pred_place, exp_place) == 1.0:\n",
    "            exact_counts[q_type] += 1\n",
    "    else:  # LinguaLocale (country only)\n",
    "        if exact_match(pred_country, exp_country) == 1.0:\n",
    "            exact_counts[q_type] += 1\n",
    "\n",
    "# 6) Compute accuracy (%) per type\n",
    "def _acc(num_exact: int, num_total: int) -> float:\n",
    "    return (num_exact / num_total) * 100 if num_total > 0 else 0.0\n",
    "\n",
    "for t in [\"GlobalTrekker\", \"CulinaryDetective\", \"LinguaLocale\"]:\n",
    "    total = type_counts.get(t, 0)\n",
    "    acc = _acc(exact_counts.get(t, 0), total)\n",
    "    print(f\"{t} Accuracy (% exact): {acc:.4f} | exact={exact_counts.get(t,0)} / total={total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e3c3b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GlobalTrekker:  P=0.0000  R=0.0000  F1=0.0000   (TP=0, FP=40, FN=62, Total positives=62)\n",
      "CulinaryDetective:  P=0.0000  R=0.0000  F1=0.0000   (TP=0, FP=36, FN=55, Total positives=55)\n",
      "LinguaLocale:  P=0.0000  R=0.0000  F1=0.0000   (TP=0, FP=40, FN=53, Total positives=53)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def calculate_metrics(tp, fp, fn):\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Containers\n",
    "tp = defaultdict(int)\n",
    "fp = defaultdict(int)\n",
    "fn = defaultdict(int)\n",
    "counts = defaultdict(int)\n",
    "\n",
    "task_types = [\"GlobalTrekker\", \"CulinaryDetective\", \"LinguaLocale\"]\n",
    "\n",
    "for (q_type, exp_country, exp_place), (p_type, pred_country, pred_place) in zip(exp, preds_raw):\n",
    "\n",
    "    for T in task_types:\n",
    "\n",
    "        # Ground truth condition\n",
    "        gt_positive = (q_type == T)\n",
    "        # Predicted condition\n",
    "        pred_positive = (p_type == T)\n",
    "\n",
    "        # Define exact match under that type\n",
    "        if T == \"GlobalTrekker\":\n",
    "            exact = (pred_country == exp_country) and (pred_place == exp_place)\n",
    "        elif T == \"CulinaryDetective\":\n",
    "            exact = (pred_country == exp_country) and (pred_place == exp_place)\n",
    "        else:  # LinguaLocale\n",
    "            exact = (pred_country == exp_country)\n",
    "\n",
    "        if gt_positive:\n",
    "            counts[T] += 1\n",
    "\n",
    "        # Evaluate classification outcomes\n",
    "        if gt_positive and pred_positive:\n",
    "            # Either TP or FN\n",
    "            if exact:\n",
    "                tp[T] += 1\n",
    "            else:\n",
    "                fn[T] += 1\n",
    "        elif (not gt_positive) and pred_positive:\n",
    "            # FP\n",
    "            fp[T] += 1\n",
    "        elif gt_positive and (not pred_positive):\n",
    "            # FN â€” model failed to predict this type\n",
    "            fn[T] += 1\n",
    "        # TN is ignored\n",
    "\n",
    "# Print results\n",
    "for T in task_types:\n",
    "    precision, recall, f1 = calculate_metrics(tp[T], fp[T], fn[T])\n",
    "    print(f\"{T}:  P={precision:.4f}  R={recall:.4f}  F1={f1:.4f}   \"\n",
    "          f\"(TP={tp[T]}, FP={fp[T]}, FN={fn[T]}, Total positives={counts[T]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "35e76631",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'country'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m answers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m questions:\n\u001b[0;32m----> 3\u001b[0m     answers\u001b[38;5;241m.\u001b[39mappend((q[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[43mq\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcountry\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, q\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m\"\u001b[39m, q\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpublic.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      5\u001b[0m     preds \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mreadlines()]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'country'"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "for q in questions:\n",
    "    answers.append((q[\"type\"], q[\"country\"], q.get(\"city\", q.get(\"region\", \"\"))))\n",
    "with open(\"public.txt\", \"r\") as f:\n",
    "    preds = [line.split(\"\\t\") for line in f.readlines()]\n",
    "\n",
    "scores = {\"GlobalTrekker\": [], \"CulinaryDetective\": [], \"LinguaLocale\": []}\n",
    "for (q_type, exp_country, exp_place), (p_type, pred_country, pred_place) in zip(answers, preds):\n",
    "    assert q_type == p_type\n",
    "    country_score = soft_match(pred_country, exp_country)\n",
    "    category_score = 0.0\n",
    "    weights = [0.0, 0.0]\n",
    "    if q_type == \"GlobalTrekker\":\n",
    "        #  correct country -> 80%, correct country and city -> +20%\n",
    "        weights = [0.8, 0.2]\n",
    "        if country_score > 0:\n",
    "            if exp_place == \"None\": category_score = 1.0\n",
    "            else: category_score = soft_match(pred_place, exp_place)\n",
    "    elif q_type == \"CulinaryDetective\":\n",
    "        # correct country -> 60%, correct country and region -> +40%\n",
    "        weights = [0.6, 0.4]\n",
    "        if country_score > 0:\n",
    "            if exp_place == \"None\": category_score = 1.0\n",
    "            else: category_score = exact_match(pred_place, exp_place)\n",
    "    else: # LinguaLocale\n",
    "        # correct country -> 60%, matched official language -> +40%\n",
    "        weights = [0.6, 0.4]\n",
    "        if country_score > 0:\n",
    "            category_score = 1.0\n",
    "        else: # incorrect country. language match works only if pred_country is a clean answer\n",
    "            exp_langs = list_of_countries.get(exp_country, [])\n",
    "            pred_langs = list_of_countries.get(pred_country, [])\n",
    "            if any(lang in exp_langs for lang in pred_langs):\n",
    "                category_score = 1.0\n",
    "\n",
    "    score = weights[0] * country_score + weights[1] * category_score\n",
    "    scores[q_type].append(score)\n",
    "\n",
    "for q_type, score_list in scores.items():\n",
    "    avg_score = sum(score_list) / len(score_list)\n",
    "    print(f\"{q_type} Average Score: {avg_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Universal Environment",
   "language": "python",
   "name": "ml-universal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
